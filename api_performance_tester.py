"""
API æ€§èƒ½æµ‹è¯•å·¥å…·

ç”¨äºæµ‹è¯• LLM API çš„å¹¶å‘æ€§èƒ½ï¼Œæ”¯æŒ SSE æµå¼è¯·æ±‚ï¼Œ
ç»Ÿè®¡ TTFTï¼ˆé¦– token æ—¶é—´ï¼‰ã€å®Œæˆæ—¶é—´ã€tokens/s ç­‰æŒ‡æ ‡ã€‚
æ”¯æŒ OpenAIã€Anthropic å’Œ Gemini API æ ¼å¼ã€‚

ä½œè€…: Claude
ç‰ˆæœ¬: 1.0.1
"""

import os
import sys
import argparse
import requests
import time
import concurrent.futures
from datetime import datetime
import statistics
import json
import random
import re

try:
    from tokenizers import Tokenizer
    TOKENIZERS_AVAILABLE = True
except ImportError:
    TOKENIZERS_AVAILABLE = False

try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False


# é»˜è®¤é…ç½®å€¼ï¼ˆä»…ä½œä¸º argparse çš„é»˜è®¤å€¼ä½¿ç”¨ï¼‰
DEFAULT_API_URL = "https://open.bigmodel.cn/api/anthropic/v1/messages"
DEFAULT_CHAT_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
DEFAULT_MODEL = "glm-4.5"
DEFAULT_TEST_MESSAGE = "What opportunities and challenges will the Chinese large model industry face in 2025?"
DEFAULT_MIN_CONCURRENCY = 5
DEFAULT_MAX_CONCURRENCY = 100
DEFAULT_STEP = 5
DEFAULT_TEST_ROUNDS = 1
DEFAULT_TIMEOUT = 120
DEFAULT_PRINT_SAMPLE_ERRORS = 5
DEFAULT_CHARS_PER_TOKEN = 4.0
DEFAULT_PROMPT_TOKENS = 500


class APIPerformanceTester:
    """API æ€§èƒ½æµ‹è¯•å·¥å…· - æ”¯æŒ SSE æµå¼è¯·æ±‚æµ‹è¯•"""
    
    def __init__(self, api_url=None, api_key=None, model=None, test_message=None, 
                 min_concurrency=None, max_concurrency=None, step=None, test_rounds=None,
                 timeout=None, print_sample_errors=None, estimate_tokens_by_chars=None,
                 chars_per_token=None, use_chat_api=None, use_stream=None, use_gemini_api=None,
                 prompt_tokens=None):
        """åˆå§‹åŒ–æµ‹è¯•é…ç½®
        
        Args:
            api_url: API åœ°å€
            api_key: API å¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹
            test_message: æµ‹è¯•æ¶ˆæ¯
            min_concurrency: æœ€å°å¹¶å‘çº§åˆ«
            max_concurrency: æœ€å¤§å¹¶å‘çº§åˆ«
            step: å¹¶å‘æ­¥é•¿
            test_rounds: æµ‹è¯•è½®æ•°
            timeout: è¶…æ—¶æ—¶é—´
            print_sample_errors: æ‰“å°é”™è¯¯æ•°é‡
            estimate_tokens_by_chars: æ˜¯å¦ä¼°ç®— tokens
            chars_per_token: å­—ç¬¦/token æ¯”ç‡
            use_chat_api: æ˜¯å¦ä½¿ç”¨ Chat API æ¥å£
            use_stream: æ˜¯å¦ä½¿ç”¨æµå¼è¯·æ±‚
            use_gemini_api: æ˜¯å¦ä½¿ç”¨ Gemini API æ¥å£
            prompt_tokens: æç¤ºè¯çš„ token æ•°é‡ï¼ˆé»˜è®¤ï¼š500ï¼‰
        """
        # API é…ç½®
        self.use_chat_api = use_chat_api or False
        self.use_gemini_api = use_gemini_api or False
        if self.use_gemini_api and api_url is None:
            self.api_url = "https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent"
        elif self.use_chat_api and api_url is None:
            self.api_url = DEFAULT_CHAT_API_URL
        else:
            self.api_url = api_url or DEFAULT_API_URL
        self.api_key = api_key
        self.model = model or DEFAULT_MODEL
        self.test_message = test_message or DEFAULT_TEST_MESSAGE
        self.use_stream = use_stream if use_stream is not None else True  # é»˜è®¤å¯ç”¨æµå¼
        
        # æµ‹è¯•å‚æ•°
        self.min_concurrency = min_concurrency or DEFAULT_MIN_CONCURRENCY
        self.max_concurrency = max_concurrency or DEFAULT_MAX_CONCURRENCY
        self.step = step or DEFAULT_STEP
        self.test_rounds = test_rounds or DEFAULT_TEST_ROUNDS
        self.timeout = timeout or DEFAULT_TIMEOUT
        self.print_sample_errors = print_sample_errors or DEFAULT_PRINT_SAMPLE_ERRORS
        self.estimate_tokens_by_chars = estimate_tokens_by_chars or False
        self.chars_per_token = chars_per_token or DEFAULT_CHARS_PER_TOKEN
        self.prompt_tokens = prompt_tokens or DEFAULT_PROMPT_TOKENS
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        self.tokenizer = None
        self._init_tokenizer()

    def run_test(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
        # æ£€æŸ¥ API Key
        if not self.api_key:
            print("âŒ é”™è¯¯ï¼šè¯·å…ˆè®¾ç½® API_KEY")
            print("æç¤ºï¼šåˆ›å»ºæµ‹è¯•å™¨æ—¶ä¼ å…¥ api_key å‚æ•°")
            return None
            
        print("ğŸš€ å¼€å§‹ API å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆSSE + TTFT + tokens/sï¼‰")
        if not self.use_stream:
            print("âš ï¸  æµå¼è¯·æ±‚å·²ç¦ç”¨ï¼Œä½¿ç”¨éæµå¼æ¨¡å¼")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"API åœ°å€: {self.api_url}")
        print(f"æ¨¡å‹: {self.model}")
        print(f"æµ‹è¯•èŒƒå›´: {self.min_concurrency}-{self.max_concurrency} å¹¶å‘ (æ­¥é•¿: {self.step})")
        print(f"æ¯ä¸ªå¹¶å‘çº§åˆ«æµ‹è¯•è½®æ•°: {self.test_rounds}")
        print(f"å•è¯·æ±‚è¶…æ—¶: {self.timeout}ç§’")
        print(f"æç¤ºè¯é•¿åº¦: {self.prompt_tokens} tokens")

        results = {}

        for concurrency in range(self.min_concurrency, self.max_concurrency + 5, self.step):
            result = test_concurrency(concurrency, self)
            results[concurrency] = result

            total_req = result.success_count + result.failure_count
            succ_rate = (result.success_count / total_req) if total_req else 0
            if succ_rate < 0.8:
                print(f"\nâš ï¸  æˆåŠŸç‡ä½äº 80%ï¼Œåœæ­¢ç»§ç»­æå‡å¹¶å‘")
                break

            time.sleep(2)  # é˜²æ­¢è¿‡åº¦å‹æµ‹

        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        self._print_summary(results)
        
        # æ‰“å°æœ€å¤§å¹¶å‘ä¸Šé™
        if results:
            max_concurrency_achieved = max(results.keys())
            max_result = results[max_concurrency_achieved]
            total_req = max_result.success_count + max_result.failure_count
            success_rate = max_result.success_count / total_req if total_req else 0
            max_concurrency_count = int(max_concurrency_achieved * success_rate)
            
            print(f"\næœ€å¤§å¹¶å‘ä¸Šé™: {max_concurrency_count}")
        
        return results
    
    def _init_tokenizer(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨
        if TOKENIZERS_AVAILABLE:
            try:
                # æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨æ–‡ä»¶
                tokenizer_file = self._get_tokenizer_file()
                if tokenizer_file and os.path.exists(tokenizer_file):
                    self.tokenizer = Tokenizer.from_file(tokenizer_file)
                    print(f"âœ… ä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨: {tokenizer_file}")
                    return
            except Exception:
                pass
        
        # å¦‚æœæœ¬åœ°åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨tiktoken
        if TIKTOKEN_AVAILABLE:
            try:
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨o200k_base
                import tiktoken
                try:
                    tiktoken.get_encoding("o200k_base")
                    print(f"âœ… ä½¿ç”¨ tiktoken åˆ†è¯å™¨")
                    return
                except:
                    # å¦‚æœo200k_baseä¸å¯ç”¨ï¼Œå›é€€åˆ°æ¨¡å‹ç‰¹å®šçš„ç¼–ç å™¨
                    encoding_name = self._get_tiktoken_encoding()
                    tiktoken.get_encoding(encoding_name)
                    print(f"âœ… ä½¿ç”¨ tiktoken åˆ†è¯å™¨")
                    return
            except Exception:
                pass
        
        # å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨å­—ç¬¦ä¼°ç®—
        print(f"ğŸ“ ä½¿ç”¨å­—ç¬¦ä¼°ç®—æ¨¡å¼ï¼ˆ{self.chars_per_token} å­—ç¬¦/tokenï¼‰")
    
    def _get_tokenizer_file(self):
        """æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„"""
        # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æ¨¡å‹åˆ°åˆ†è¯å™¨æ–‡ä»¶çš„æ˜ å°„
        MODEL_TOKENIZER_MAP = {
            "glm": "tokenizer_glm.json",
            "glm-4": "tokenizer_glm.json",
            "glm-4.5": "tokenizer_glm.json",
            "deepseek": "tokenizer_ds.json",
            "deepseek-chat": "tokenizer_ds.json",
            "deepseek-coder": "tokenizer_ds.json",
            "llama": "tokenizer_llama.json",
            "llama-2": "tokenizer_llama.json",
            "llama-3": "tokenizer_llama.json",
            "llama-3.1": "tokenizer_llama.json",
            "grok": "tokenizer_grok2.json",
            "grok-2": "tokenizer_grok2.json",
            "grok-beta": "tokenizer_grok2.json"
        }
        
        # æ£€æŸ¥æ¨¡å‹åç§°ï¼ˆè½¬ä¸ºå°å†™è¿›è¡ŒåŒ¹é…ï¼‰
        model_lower = self.model.lower()
        
        # éå†æ˜ å°„è¡¨ï¼Œæ‰¾åˆ°åŒ¹é…çš„åˆ†è¯å™¨
        for model_key, tokenizer_filename in MODEL_TOKENIZER_MAP.items():
            if model_key in model_lower:
                tokenizer_path = os.path.join(current_dir, tokenizer_filename)
                return tokenizer_path
        
        return None
    
    def _get_tiktoken_encoding(self):
        """æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„tiktokenç¼–ç å™¨åç§°"""
        # æ¨¡å‹åˆ°tiktokenç¼–ç å™¨çš„æ˜ å°„ï¼ˆç®€åŒ–ç‰ˆï¼‰
        MODEL_TIKTOKEN_MAP = {
            "gpt-4o": "o200k_base",
            "gpt-4o-mini": "o200k_base",
            "gpt-4": "cl100k_base",
            "gpt-3.5": "cl100k_base",
            "claude": "cl100k_base",
            "deepseek": "cl100k_base",
            "mistral": "cl100k_base",
            "llama": "cl100k_base",
            "grok": "cl100k_base",
            "qwen": "cl100k_base",
            "default": "o200k_base"
        }
        
        # æ£€æŸ¥æ¨¡å‹åç§°ï¼ˆè½¬ä¸ºå°å†™è¿›è¡ŒåŒ¹é…ï¼‰
        model_lower = self.model.lower()
        
        # éå†æ˜ å°„è¡¨ï¼Œæ‰¾åˆ°åŒ¹é…çš„ç¼–ç å™¨
        for model_key, encoding_name in MODEL_TIKTOKEN_MAP.items():
            if model_key in model_lower:
                return encoding_name
        
        # ä½¿ç”¨é»˜è®¤ç¼–ç å™¨
        return MODEL_TIKTOKEN_MAP.get("default", "o200k_base")
    
    def _count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨
        if self.tokenizer:
            try:
                encoding = self.tokenizer.encode(text)
                return len(encoding.ids)
            except Exception:
                pass
        
        # å¦‚æœæœ¬åœ°åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨tiktoken
        if TIKTOKEN_AVAILABLE:
            try:
                import tiktoken
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨o200k_base
                try:
                    encoding = tiktoken.get_encoding("o200k_base")
                    return len(encoding.encode(text))
                except:
                    # å¦‚æœo200k_baseä¸å¯ç”¨ï¼Œå›é€€åˆ°æ¨¡å‹ç‰¹å®šçš„ç¼–ç å™¨
                    encoding_name = self._get_tiktoken_encoding()
                    encoding = tiktoken.get_encoding(encoding_name)
                    return len(encoding.encode(text))
            except Exception:
                pass
        
        # æœ€åå›é€€åˆ°å­—ç¬¦ä¼°ç®—
        return int(len(text) / self.chars_per_token)
    
    def _generate_prompt_content(self, target_tokens):
        """ç”ŸæˆæŒ‡å®š token æ•°é‡çš„æç¤ºå†…å®¹"""
        # åŸºç¡€é—®é¢˜æ¨¡æ¿
        base_question = "What opportunities and challenges will the Chinese large model industry face in 2025? Please analyze from the following aspects:"
        
        # åˆ†ææ–¹é¢åˆ—è¡¨
        aspects = [
            "1. Technical development trends and breakthrough points",
            "2. Policy environment and regulatory challenges",
            "3. Commercial application scenarios and market prospects",
            "4. Data security and privacy protection issues",
            "5. International competition and cooperation landscape",
            "6. Talent cultivation and ecosystem construction",
            "7. Computing power and infrastructure constraints",
            "8. Ethical considerations and social responsibility",
            "9. Industry-specific applications and customization needs",
            "10. Future development predictions and strategic recommendations"
        ]
        
        # ç”Ÿæˆå†…å®¹
        content = base_question + "\n\n" + "\n".join(aspects)
        
        # è®¡ç®—å½“å‰tokenæ•°
        current_tokens = self._count_tokens(content)
        
        # å¦‚æœå½“å‰tokenæ•°ä¸è¶³ï¼Œæ·»åŠ æ›´å¤šå†…å®¹
        if current_tokens < target_tokens:
            # ç”Ÿæˆé¢å¤–çš„è¯¦ç»†å†…å®¹
            additional_content = []
            paragraph_num = 0
            
            while current_tokens < target_tokens and paragraph_num < 100:
                # ç”Ÿæˆæ®µè½å†…å®¹
                paragraph = self._generate_detailed_paragraph(paragraph_num)
                paragraph_tokens = self._count_tokens(paragraph)
                
                if current_tokens + paragraph_tokens <= target_tokens:
                    additional_content.append(paragraph)
                    current_tokens += paragraph_tokens
                    paragraph_num += 1
                else:
                    # éœ€è¦ç²¾ç¡®æ§åˆ¶é•¿åº¦
                    remaining_tokens = target_tokens - current_tokens
                    if remaining_tokens > 50:  # åªæœ‰å½“å‰©ä½™tokenè¶³å¤Ÿæ—¶æ‰æ·»åŠ 
                        # æˆªå–æ®µè½çš„ä¸€éƒ¨åˆ†
                        partial_text = self._extract_partial_text(paragraph, remaining_tokens)
                        if partial_text:
                            additional_content.append(partial_text)
                            break
                    break
            
            if additional_content:
                content += "\n\n" + "\n\n".join(additional_content)
        
        # æœ€ç»ˆæ£€æŸ¥å’Œè°ƒæ•´
        final_tokens = self._count_tokens(content)
        if final_tokens != target_tokens:
            # ç®€å•çš„å­—ç¬¦çº§è°ƒæ•´
            content = self._adjust_content_length(content, target_tokens)
        
        return content
    
    def _generate_detailed_paragraph(self, num):
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææ®µè½"""
        # é¢„å®šä¹‰çš„è¯¦ç»†åˆ†æå†…å®¹æ¨¡æ¿
        templates = [
            "The Chinese large model industry is experiencing rapid development, with significant investments in research and development. Key players are focusing on improving model capabilities while reducing computational costs.",
            "Market analysis shows that domestic large models have made substantial progress in various applications, including natural language processing, computer vision, and multimodal systems.",
            "Technological innovation remains a critical factor, with Chinese companies developing unique approaches to model architecture, training methodologies, and deployment strategies.",
            "The regulatory landscape continues to evolve, with authorities seeking to balance innovation promotion with necessary oversight and risk management.",
            "International cooperation and competition shape the industry's trajectory, influencing technology transfer, market access, and standard development.",
            "Talent development has become a strategic priority, with educational institutions and companies working to build a skilled workforce in AI and machine learning.",
            "Infrastructure development, particularly computing resources, presents both challenges and opportunities for industry growth and scalability.",
            "Ethical considerations and responsible AI practices are increasingly important, influencing corporate strategies and public perception.",
            "Industry-specific applications demonstrate the practical value of large models across healthcare, finance, education, and manufacturing sectors.",
            "Future prospects depend on sustained innovation, supportive policies, and the ability to address technical and commercial challenges effectively."
        ]
        
        # é€‰æ‹©æ¨¡æ¿å¹¶æ·»åŠ ç¼–å·
        template = templates[num % len(templates)]
        return f"Detailed Analysis Point {num + 1}: {template} This analysis considers multiple factors including technological maturity, market readiness, regulatory compliance, and competitive positioning. The impact on industry development could be substantial, requiring careful strategic planning and resource allocation."
    
    def _extract_partial_text(self, text, target_tokens):
        """ä»æ–‡æœ¬ä¸­æå–æŒ‡å®štokenæ•°é‡çš„éƒ¨åˆ†"""
        if self.tokenizer:
            try:
                encoding = self.tokenizer.encode(text)
                if len(encoding.ids) <= target_tokens:
                    return text
                # æˆªæ–­åˆ°ç›®æ ‡tokenæ•°
                truncated_encoding = encoding.truncate(target_tokens)
                if truncated_encoding and hasattr(truncated_encoding, 'ids') and truncated_encoding.ids:
                    return self.tokenizer.decode(truncated_encoding.ids)
            except Exception:
                pass
        
        # ä½¿ç”¨å­—ç¬¦ä¼°ç®—
        estimated_chars = int(target_tokens * self.chars_per_token)
        return text[:estimated_chars]
    
    def _adjust_content_length(self, content, target_tokens):
        """è°ƒæ•´å†…å®¹é•¿åº¦ä»¥åŒ¹é…ç›®æ ‡tokens"""
        current_tokens = self._count_tokens(content)
        
        if current_tokens <= target_tokens:
            return content
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„å­—ç¬¦æ•°
        estimated_chars = int(target_tokens * self.chars_per_token * 0.95)
        
        # åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        truncated = content[:estimated_chars]
        sentence_ends = ['. ', '! ', '? ', 'ã€‚\n', 'ï¼\n', 'ï¼Ÿ\n', '\n\n']
        best_pos = 0
        for ending in sentence_ends:
            pos = truncated.rfind(ending)
            if pos > best_pos:
                best_pos = pos + len(ending)
        
        if best_pos > 0:
            return content[:best_pos]
        else:
            return truncated
        
    def _print_summary(self, results):
        """æ‰“å°æµ‹è¯•æ±‡æ€»æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print(f"ğŸ“‹ æµ‹è¯•æ±‡æ€»æŠ¥å‘Š {self.api_url}")
        print("=" * 60)
        if self.use_stream:
            print("\nå¹¶å‘çº§åˆ« | æˆåŠŸç‡ | å¹³å‡å®Œæˆæ—¶é—´ | å¹³å‡TTFT | å¹³å‡tokens/s | tokens")
        else:
            print("\nå¹¶å‘çº§åˆ« | æˆåŠŸç‡ | å¹³å‡å®Œæˆæ—¶é—´ | å¹³å‡å“åº”æ—¶é—´ | å¹³å‡tokens/s | tokens")
        print("-" * 85)

        for concurrency, result in results.items():
            total_req = result.success_count + result.failure_count
            succ_rate = (result.success_count / total_req) * 100 if total_req else 0.0
            avg_time = statistics.mean(result.response_times) if result.response_times else float("nan")
            avg_ttft = statistics.mean(result.first_token_times) if result.first_token_times else float("nan")
            avg_tps = statistics.mean(result.tokens_per_sec) if result.tokens_per_sec else float("nan")
            print(f"{concurrency:8d} | {succ_rate:6.1f}% | {avg_time:10.2f}s | {avg_ttft:8.3f}s | {avg_tps:12.2f} | {self.prompt_tokens:6d}")


class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    
    def __init__(self):
        self.success_count = 0
        self.failure_count = 0
        self.response_times = []      # æ•´ä½“å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰
        self.first_token_times = []   # TTFTï¼ˆç§’ï¼‰
        self.tokens_generated = []    # æ¯æ¬¡è¯·æ±‚çš„è¾“å‡º token æ•°
        self.tokens_per_sec = []      # æ¯æ¬¡è¯·æ±‚ tokens/s
        self.status_codes = []
        self.errors = []


def make_request(tester=None):
    """
    å‘é€å•ä¸ª SSE æµå¼è¯·æ±‚ï¼›ç»Ÿè®¡ï¼š
      - TTFTï¼ˆé¦– token æ—¶é—´ï¼‰
      - å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰
      - è¾“å‡º token æ•°ï¼ˆä¼˜å…ˆå– message_delta.usage.output_tokensï¼›å¦åˆ™å¯é€‰ä¼°ç®—ï¼‰
      - tokens/s = è¾“å‡º token æ•° / å®Œæˆæ—¶é—´
    
    Args:
        tester: APIPerformanceTester å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å˜é‡
    """
    if tester is None:
        # ä¸æ”¯æŒæ—  tester çš„è°ƒç”¨æ–¹å¼ï¼Œå¿…é¡»ä¼ å…¥ tester
        raise ValueError("make_request å¿…é¡»ä¼ å…¥ tester å‚æ•°")
    else:
        # ä½¿ç”¨ tester å®ä¾‹çš„é…ç½®
        api_url = tester.api_url
        api_key = tester.api_key
        model = tester.model
        test_message = tester.test_message
        
        # å¦‚æœæŒ‡å®šäº†prompt_tokensä¸”ä¸ä½¿ç”¨é»˜è®¤æ¶ˆæ¯ï¼Œåˆ™ç”ŸæˆæŒ‡å®štokenæ•°é‡çš„å†…å®¹
        if hasattr(tester, 'prompt_tokens') and tester.prompt_tokens != DEFAULT_PROMPT_TOKENS:
            test_message = tester._generate_prompt_content(tester.prompt_tokens)
            # actual_tokens = tester._count_tokens(test_message)
            # print(f"ğŸ“ ç”Ÿæˆäº† {actual_tokens} tokens çš„æç¤ºå†…å®¹ï¼ˆç›®æ ‡ï¼š{tester.prompt_tokens}ï¼‰")
        timeout = tester.timeout
        estimate_tokens_by_chars = tester.estimate_tokens_by_chars
        chars_per_token = tester.chars_per_token
        
    start_time = time.time()
    first_token_time = None
    output_tokens = None  # æ¥è‡ª message_delta çš„ usage.output_tokensï¼ˆç´¯è®¡ï¼‰
    approx_chars = 0      # å¦‚æœéœ€è¦ä¼°ç®—æ—¶ä½¿ç”¨

    # æ£€æµ‹æ˜¯å¦ä¸º Cerebras API
    is_cerebras_api = "api.cerebras.ai" in api_url.lower()
    
    # æ£€æµ‹æ˜¯å¦ä¸º Gemini API
    is_gemini_api = "generativelanguage.googleapis.com" in api_url.lower()
    
    # å¤„ç† Gemini API URL ä¸­çš„æ¨¡å‹å ä½ç¬¦
    if is_gemini_api and "{model}" in api_url:
        api_url = api_url.replace("{model}", model)
    
    # æ ¹æ®æ¥å£ç±»å‹è®¾ç½®ä¸åŒçš„ headers å’Œ payload
    if is_gemini_api:
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key,
        }
        payload = {
            "contents": [{
                "role": "user",
                "parts": [{"text": test_message}]
            }],
            "generationConfig": {
                "maxOutputTokens": 1024,
                "temperature": 0.2,
            }
        }
        # Gemini API æ€»æ˜¯ä½¿ç”¨æµå¼
        payload["stream"] = True
    elif tester.use_chat_api or is_cerebras_api:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": model,
            "temperature": 0.2,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„åŠ©æ‰‹ï¼Œåªè¿”å›æœ€ç»ˆç­”æ¡ˆã€‚"},
                {"role": "user", "content": test_message}
            ]
        }
        
        # Cerebras API ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–ä½¿ç”¨ max_tokens
        if is_cerebras_api:
            payload["max_completion_tokens"] = 1024
        else:
            payload["max_tokens"] = 1024
            
        if tester.use_stream:
            payload["stream"] = True
    else:
        headers = {
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json",
        }
        payload = {
            "model": model,
            "max_tokens": 1024,
            "temperature": 0.2,
            "messages": [
                {"role": "user", "content": test_message}
            ]
        }
        if tester.use_stream:
            payload["stream"] = True

    try:
        with requests.post(
            api_url,
            headers=headers,
            data=json.dumps(payload),
            stream=tester.use_stream,  # æ ¹æ® use_stream å†³å®šæ˜¯å¦ä½¿ç”¨æµå¼
            timeout=timeout,
        ) as r:
            status = r.status_code
            if status != 200:
                total_time = time.time() - start_time
                text = r.text[:200] if r.text else ""
                return (False, total_time, status, f"HTTP {status}: {text}", None, None, None)

            # å¤„ç†éæµå¼å“åº”
            if not tester.use_stream:
                total_time = time.time() - start_time
                response_data = r.json()
                
                # æ ¹æ®æ¥å£ç±»å‹è§£æä¸åŒçš„å“åº”æ ¼å¼
                if is_gemini_api:
                    # Gemini API éæµå¼å“åº”
                    candidates = response_data.get("candidates", [])
                    if candidates:
                        candidate = candidates[0]
                        content = candidate.get("content", {})
                        parts = content.get("parts", [])
                        text = ""
                        for part in parts:
                            if "text" in part:
                                text += part["text"]
                        
                        # Gemini ä¸æä¾› usage ä¿¡æ¯ï¼Œéœ€è¦ä¼°ç®—
                        if estimate_tokens_by_chars:
                            output_tokens = max(1, int(len(text) / chars_per_token))
                elif tester.use_chat_api or is_cerebras_api:
                    # Chat API æ ¼å¼ (åŒ…æ‹¬ Cerebras)
                    usage = response_data.get("usage", {})
                    output_tokens = usage.get("completion_tokens")
                    content = ""
                    choices = response_data.get("choices", [])
                    if choices:
                        message = choices[0].get("message", {})
                        content = message.get("content", "")
                else:
                    # Anthropic API æ ¼å¼
                    usage = response_data.get("usage", {})
                    output_tokens = usage.get("output_tokens")
                    content = ""
                    content_blocks = response_data.get("content", [])
                    for block in content_blocks:
                        if block.get("type") == "text":
                            content += block.get("text", "")
                
                # å¦‚æœæ²¡æœ‰è·å–åˆ° token æ•°ï¼ŒæŒ‰éœ€ä¼°ç®—
                if output_tokens is None and estimate_tokens_by_chars:
                    output_tokens = max(1, int(len(content) / chars_per_token))
                
                # è®¡ç®— tokens/s
                tokens_per_sec = None
                if output_tokens is not None and total_time > 0:
                    tokens_per_sec = output_tokens / total_time
                
                return (True, total_time, status, None, total_time, output_tokens, tokens_per_sec)

            # å¤„ç†æµå¼å“åº”
            for raw_line in r.iter_lines(decode_unicode=True):
                if not raw_line or not raw_line.startswith("data:"):
                    continue

                chunk = raw_line[len("data:"):].strip()
                if not chunk:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæµç»“æŸæ ‡è®°
                if chunk == "[DONE]":
                    total_time = time.time() - start_time
                    if first_token_time is None:
                        first_token_time = total_time
                    
                    # è‹¥æœªæ‹¿åˆ° tokensï¼ŒæŒ‰éœ€ä¼°ç®—
                    if output_tokens is None and estimate_tokens_by_chars:
                        output_tokens = max(1, int(approx_chars / chars_per_token))
                    
                    # è®¡ç®— tokens/s
                    tokens_per_sec = None
                    if output_tokens is not None and total_time > 0:
                        tokens_per_sec = output_tokens / total_time
                    
                    return (True, total_time, status, None, first_token_time, output_tokens, tokens_per_sec)

                try:
                    event = json.loads(chunk)
                except json.JSONDecodeError:
                    continue

                # æ ¹æ®æ¥å£ç±»å‹è§£æä¸åŒçš„å“åº”æ ¼å¼
                if is_gemini_api:
                    # Gemini API æµå¼å“åº”
                    candidates = event.get("candidates", [])
                    if candidates:
                        candidate = candidates[0]
                        content = candidate.get("content", {})
                        parts = content.get("parts", [])
                        
                        for part in parts:
                            if "text" in part and part.get("text"):
                                # è®°å½•é¦– token æ—¶é—´
                                if first_token_time is None:
                                    first_token_time = time.time() - start_time
                                if estimate_tokens_by_chars:
                                    approx_chars += len(part.get("text", ""))
                        
                        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                        finish_reason = candidate.get("finishReason")
                        if finish_reason in ["STOP", "MAX_TOKENS", "SAFETY", "RECITATION"]:
                            total_time = time.time() - start_time
                            if first_token_time is None:
                                first_token_time = total_time
                            
                            # Gemini ä¸æä¾› usage ä¿¡æ¯ï¼Œéœ€è¦ä¼°ç®—
                            if output_tokens is None and estimate_tokens_by_chars:
                                output_tokens = max(1, int(approx_chars / chars_per_token))
                            
                            # è®¡ç®— tokens/s
                            tokens_per_sec = None
                            if output_tokens is not None and total_time > 0:
                                tokens_per_sec = output_tokens / total_time
                            
                            return (True, total_time, status, None, first_token_time, output_tokens, tokens_per_sec)
                elif tester.use_chat_api or is_cerebras_api:
                    # Chat API æ ¼å¼ (åŒ…æ‹¬ Cerebras)
                    choices = event.get("choices", [])
                    if choices:
                        choice = choices[0]
                        delta = choice.get("delta", {})
                        
                        # è®°å½•é¦– token æ—¶é—´ï¼ˆç¬¬ä¸€ä¸ª content å‡ºç°ï¼‰
                        if "content" in delta and delta.get("content"):
                            if first_token_time is None:
                                first_token_time = time.time() - start_time
                            if estimate_tokens_by_chars:
                                approx_chars += len(delta.get("content", ""))
                        
                        # è·å– usage ä¿¡æ¯
                        usage = event.get("usage")
                        if usage and "completion_tokens" in usage:
                            output_tokens = usage.get("completion_tokens")
                    
                    # æ£€æŸ¥æ˜¯å¦ç»“æŸ - æ”¯æŒå¤šç§ç»“æŸæ¡ä»¶
                    finish_reason = None
                    if choices:
                        finish_reason = choice.get("finish_reason")
                    
                    if finish_reason in ["stop", "length", "content_filter", "function_call"]:
                        total_time = time.time() - start_time
                        if first_token_time is None:
                            first_token_time = total_time
                        
                        # è‹¥æœªæ‹¿åˆ° usage.completion_tokensï¼ŒæŒ‰éœ€ä¼°ç®—
                        if output_tokens is None and estimate_tokens_by_chars:
                            output_tokens = max(1, int(approx_chars / chars_per_token))
                        
                        # è®¡ç®— tokens/s
                        tokens_per_sec = None
                        if output_tokens is not None and total_time > 0:
                            tokens_per_sec = output_tokens / total_time
                        
                        return (True, total_time, status, None, first_token_time, output_tokens, tokens_per_sec)
                else:
                    # Anthropic API æ ¼å¼
                    etype = event.get("type")

                    # è®°å½•é¦– token æ—¶é—´ï¼ˆç¬¬ä¸€æ®µ text_delta å‡ºç°ï¼‰
                    if etype == "content_block_delta":
                        delta = event.get("delta", {})
                        if delta.get("type") == "text_delta":
                            if first_token_time is None:
                                first_token_time = time.time() - start_time
                            if estimate_tokens_by_chars:
                                approx_chars += len(delta.get("text", ""))

                    # usage ç´¯åŠ é€šå¸¸åœ¨ message_delta äº‹ä»¶é‡Œ
                    if etype == "message_delta":
                        usage = event.get("usage") or {}
                        # ä¸€èˆ¬æ˜¯ç´¯è®¡å€¼ï¼ˆåˆ°å½“å‰ä¸ºæ­¢çš„è¾“å‡º token æ•°ï¼‰
                        if "output_tokens" in usage:
                            output_tokens = usage.get("output_tokens")

                    if etype == "message_stop":
                        total_time = time.time() - start_time
                        if first_token_time is None:
                            first_token_time = total_time  # æç«¯æƒ…å†µï¼šå‡ ä¹æ— è¾“å‡º

                        # è‹¥æœªæ‹¿åˆ° usage.output_tokensï¼ŒæŒ‰éœ€ä¼°ç®—
                        if output_tokens is None and estimate_tokens_by_chars:
                            output_tokens = max(1, int(approx_chars / chars_per_token))

                        # è®¡ç®— tokens/s
                        tokens_per_sec = None
                        if output_tokens is not None and total_time > 0:
                            tokens_per_sec = output_tokens / total_time

                        return (True, total_time, status, None, first_token_time, output_tokens, tokens_per_sec)

            # æœªæ”¶åˆ°ç»“æŸæ ‡å¿—
            total_time = time.time() - start_time
            if is_gemini_api:
                return (False, total_time, status, "Stream ended without finishReason", first_token_time, output_tokens, None)
            elif tester.use_chat_api:
                return (False, total_time, status, "Stream ended without finish_reason=stop", first_token_time, output_tokens, None)
            else:
                return (False, total_time, status, "Stream ended without message_stop", first_token_time, output_tokens, None)

    except Exception as e:
        total_time = time.time() - start_time
        return (False, total_time, None, str(e), first_token_time, output_tokens, None)


def test_concurrency(concurrency_level, tester=None):
    """æµ‹è¯•æŒ‡å®šå¹¶å‘çº§åˆ«ï¼ˆSSE + TTFT + tokens/sï¼‰
    
    Args:
        concurrency_level: å¹¶å‘çº§åˆ«
        tester: APIPerformanceTester å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€é…ç½®
    """
    print(f"\nğŸ”„ æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency_level}")
    print("=" * 50)
    
    # æ˜¾ç¤ºæç¤ºè¯tokenä¿¡æ¯
    if hasattr(tester, 'prompt_tokens') and tester.prompt_tokens != DEFAULT_PROMPT_TOKENS:
        print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {tester.prompt_tokens} tokens")
    elif hasattr(tester, 'prompt_tokens'):
        print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {tester.prompt_tokens} tokens (é»˜è®¤)")

    result = TestResult()

    test_rounds = tester.test_rounds
    
    for round_num in range(test_rounds):
        print(f"   ç¬¬ {round_num + 1}/{test_rounds} è½®æµ‹è¯•...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency_level) as executor:
            futures = [executor.submit(make_request, tester) for _ in range(concurrency_level)]

            for future in concurrent.futures.as_completed(futures):
                success, total_time, status_code, error, ttft, out_tokens, tps = future.result()

                if success:
                    result.success_count += 1
                else:
                    result.failure_count += 1
                    if error:
                        result.errors.append(error)

                result.response_times.append(total_time)
                if ttft is not None:
                    result.first_token_times.append(ttft)
                if out_tokens is not None:
                    result.tokens_generated.append(out_tokens)
                if tps is not None:
                    result.tokens_per_sec.append(tps)
                if status_code:
                    result.status_codes.append(status_code)

    # ç»Ÿè®¡
    total_requests = result.success_count + result.failure_count
    success_rate = (result.success_count / total_requests) * 100 if total_requests else 0.0

    def safe_mean(xs): return statistics.mean(xs) if xs else float("nan")
    def safe_min(xs):  return min(xs) if xs else float("nan")
    def safe_max(xs):  return max(xs) if xs else float("nan")

    def percentile(xs, p):
        if not xs:
            return float("nan")
        xs_sorted = sorted(xs)
        k = (len(xs_sorted) - 1) * p
        f = int(k)
        c = min(f + 1, len(xs_sorted) - 1)
        if f == c:
            return xs_sorted[f]
        return xs_sorted[f] + (xs_sorted[c] - xs_sorted[f]) * (k - f)

    avg_response_time = safe_mean(result.response_times)
    min_response_time = safe_min(result.response_times)
    max_response_time = safe_max(result.response_times)

    avg_ttft = safe_mean(result.first_token_times)
    p50_ttft = percentile(result.first_token_times, 0.5)
    p95_ttft = percentile(result.first_token_times, 0.95)

    avg_tokens = safe_mean(result.tokens_generated)
    sum_tokens = sum(result.tokens_generated) if result.tokens_generated else 0
    avg_tps = safe_mean(result.tokens_per_sec)
    p50_tps = percentile(result.tokens_per_sec, 0.5)
    p95_tps = percentile(result.tokens_per_sec, 0.95)
    max_tps = safe_max(result.tokens_per_sec)

    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æˆåŠŸ: {result.success_count} | å¤±è´¥: {result.failure_count}")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"   å¹³å‡å®Œæˆæ—¶é—´: {avg_response_time:.2f}s  (æœ€å¿« {min_response_time:.2f}s | æœ€æ…¢ {max_response_time:.2f}s)")
    if tester.use_stream:
        print(f"   TTFT(é¦–å­—å“åº”): å¹³å‡ {avg_ttft:.3f}s | P50 {p50_ttft:.3f}s | P95 {p95_ttft:.3f}s")
    else:
        print(f"   å“åº”æ—¶é—´(TTFB): å¹³å‡ {avg_ttft:.3f}s | P50 {p50_ttft:.3f}s | P95 {p95_ttft:.3f}s")
    
    if result.tokens_per_sec:
        print(f"   è¾“å‡ºToken: æ€»è®¡ {sum_tokens} | å•æ¬¡å¹³å‡ {avg_tokens:.1f}")
        print(f"   è¾“å‡ºé€Ÿç‡(tokens/s): å¹³å‡ {avg_tps:.2f} | P50 {p50_tps:.2f} | P95 {p95_tps:.2f} | æœ€é«˜ {max_tps:.2f}")
    else:
        if tester.use_gemini_api:
            print("   âš ï¸ Gemini API ä¸æä¾› usage ä¿¡æ¯ï¼›å¦‚éœ€ä¼°ç®— tokensï¼Œè¯·ä½¿ç”¨ --estimate-tokens å‚æ•°ã€‚")
        elif tester.use_chat_api:
            print("   âš ï¸ æœªè·å–åˆ° usage.completion_tokensï¼›å¦‚éœ€ä¼°ç®—ï¼Œè¯·ä½¿ç”¨ --estimate-tokens å‚æ•°ã€‚")
        else:
            print("   âš ï¸ æœªè·å–åˆ° usage.output_tokensï¼›å¦‚éœ€ä¼°ç®—ï¼Œè¯·ä½¿ç”¨ --estimate-tokens å‚æ•°ã€‚")

    if result.errors:
        print_limit = tester.print_sample_errors
        print(f"\nâŒ é”™è¯¯æ±‡æ€» (å‰{print_limit}ä¸ª):")
        for i, error in enumerate(result.errors[:print_limit], 1):
            print(f"   {i}. {error}")

    return result


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="API æ€§èƒ½æµ‹è¯•å·¥å…· - æµ‹è¯• LLM API çš„å¹¶å‘æ€§èƒ½ï¼ˆæ”¯æŒ OpenAIã€Anthropic å’Œ Gemini æ ¼å¼ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python api_performance_tester.py --key your_api_key_here
  python api_performance_tester.py --key your_api_key_here --model glm-4-0528
  python api_performance_tester.py --key your_api_key_here --min 5 --max 50 --step 5
  python api_performance_tester.py --key your_api_key_here --rounds 3 --timeout 60
  python api_performance_tester.py --key your_api_key_here --chat-api  # ä½¿ç”¨ Chat API æ¥å£
  python api_performance_tester.py --key your_api_key_here --no-stream  # ç¦ç”¨æµå¼è¯·æ±‚
  python api_performance_tester.py --key your_api_key_here --gemini-api  # ä½¿ç”¨ Gemini API æ¥å£
  python api_performance_tester.py --key your_api_key_here --prompt-tokens 1000  # ä½¿ç”¨1000 tokensçš„æç¤ºè¯
        """
    )
    
    # API é…ç½®å‚æ•°
    parser.add_argument(
        "--key", 
        required=True,
        help="API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰"
    )
    parser.add_argument(
        "--url",
        default=DEFAULT_API_URL,
        help="API æ¥å£åœ°å€ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    parser.add_argument(
        "--message",
        default=DEFAULT_TEST_MESSAGE,
        help="æµ‹è¯•æ¶ˆæ¯å†…å®¹"
    )
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument(
        "--min",
        type=int,
        default=DEFAULT_MIN_CONCURRENCY,
        help="æœ€å°å¹¶å‘çº§åˆ«ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--max",
        type=int,
        default=DEFAULT_MAX_CONCURRENCY,
        help="æœ€å¤§å¹¶å‘çº§åˆ«ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--step",
        type=int,
        default=DEFAULT_STEP,
        help="å¹¶å‘çº§åˆ«æ­¥é•¿ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--rounds",
        type=int,
        default=DEFAULT_TEST_ROUNDS,
        help="æ¯ä¸ªå¹¶å‘çº§åˆ«æµ‹è¯•è½®æ•°ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=DEFAULT_TIMEOUT,
        help="å•è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--estimate-tokens",
        action="store_true",
        help="ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®— tokensï¼ˆé»˜è®¤ï¼šä¸å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--chars-per-token",
        type=float,
        default=DEFAULT_CHARS_PER_TOKEN,
        help="æ¯ä¸ª token çš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š%(default).1fï¼‰"
    )
    parser.add_argument(
        "--chat-api",
        action="store_true",
        help="ä½¿ç”¨ Chat API æ¥å£ï¼ˆé»˜è®¤ï¼šä½¿ç”¨ Anthropic æ¥å£ï¼‰"
    )
    parser.add_argument(
        "--no-stream",
        action="store_true",
        help="ç¦ç”¨æµå¼è¯·æ±‚ï¼ˆé»˜è®¤ï¼šå¯ç”¨æµå¼ï¼‰"
    )
    parser.add_argument(
        "--gemini-api",
        action="store_true",
        help="ä½¿ç”¨ Gemini API æ¥å£ï¼ˆé»˜è®¤ï¼šä¸ä½¿ç”¨ï¼‰"
    )
    parser.add_argument(
        "--prompt-tokens",
        type=int,
        default=DEFAULT_PROMPT_TOKENS,
        help="æç¤ºè¯çš„ token æ•°é‡ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    # å¦‚æœä½¿ç”¨ Gemini API ä¸”æœªæŒ‡å®š URLï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„ Gemini API URL
    api_url = args.url
    if args.gemini_api and api_url == DEFAULT_API_URL:
        api_url = "https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent"
    elif args.chat_api and api_url == DEFAULT_API_URL:
        api_url = DEFAULT_CHAT_API_URL
    
    tester = APIPerformanceTester(
        api_url=api_url,
        api_key=args.key,
        model=args.model,
        test_message=args.message,
        min_concurrency=args.min,
        max_concurrency=args.max,
        step=args.step,
        test_rounds=args.rounds,
        timeout=args.timeout,
        estimate_tokens_by_chars=args.estimate_tokens,
        chars_per_token=args.chars_per_token,
        use_chat_api=args.chat_api,
        use_stream=not args.no_stream,
        use_gemini_api=args.gemini_api,
        prompt_tokens=args.prompt_tokens
    )
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_test()
    
    return results


if __name__ == "__main__":
    main()
