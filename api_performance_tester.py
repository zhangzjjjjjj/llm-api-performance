"""
API æ€§èƒ½æµ‹è¯•å·¥å…·

ç”¨äºæµ‹è¯• BigModel API çš„å¹¶å‘æ€§èƒ½ï¼Œæ”¯æŒ SSE æµå¼è¯·æ±‚ï¼Œ
ç»Ÿè®¡ TTFTï¼ˆé¦– token æ—¶é—´ï¼‰ã€å®Œæˆæ—¶é—´ã€tokens/s ç­‰æŒ‡æ ‡ã€‚

ä½œè€…: Claude
ç‰ˆæœ¬: 1.0.0
"""

import os
import sys
import argparse
import requests
import time
import concurrent.futures
from datetime import datetime
import statistics
import json


# é»˜è®¤é…ç½®å€¼ï¼ˆä»…ä½œä¸º argparse çš„é»˜è®¤å€¼ä½¿ç”¨ï¼‰
DEFAULT_API_URL = "https://open.bigmodel.cn/api/anthropic/v1/messages"
DEFAULT_MODEL = "glm-4.5"
DEFAULT_TEST_MESSAGE = "What opportunities and challenges will the Chinese large model industry face in 2025?"
DEFAULT_MIN_CONCURRENCY = 5
DEFAULT_MAX_CONCURRENCY = 100
DEFAULT_STEP = 5
DEFAULT_TEST_ROUNDS = 1
DEFAULT_TIMEOUT = 120
DEFAULT_PRINT_SAMPLE_ERRORS = 5
DEFAULT_CHARS_PER_TOKEN = 4.0


class APIPerformanceTester:
    """API æ€§èƒ½æµ‹è¯•å·¥å…· - æ”¯æŒ SSE æµå¼è¯·æ±‚æµ‹è¯•"""
    
    def __init__(self, api_url=None, api_key=None, model=None, test_message=None, 
                 min_concurrency=None, max_concurrency=None, step=None, test_rounds=None,
                 timeout=None, print_sample_errors=None, estimate_tokens_by_chars=None,
                 chars_per_token=None):
        """åˆå§‹åŒ–æµ‹è¯•é…ç½®
        
        Args:
            api_url: API åœ°å€
            api_key: API å¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹
            test_message: æµ‹è¯•æ¶ˆæ¯
            min_concurrency: æœ€å°å¹¶å‘çº§åˆ«
            max_concurrency: æœ€å¤§å¹¶å‘çº§åˆ«
            step: å¹¶å‘æ­¥é•¿
            test_rounds: æµ‹è¯•è½®æ•°
            timeout: è¶…æ—¶æ—¶é—´
            print_sample_errors: æ‰“å°é”™è¯¯æ•°é‡
            estimate_tokens_by_chars: æ˜¯å¦ä¼°ç®— tokens
            chars_per_token: å­—ç¬¦/token æ¯”ç‡
        """
        # API é…ç½®
        self.api_url = api_url or DEFAULT_API_URL
        self.api_key = api_key
        self.model = model or DEFAULT_MODEL
        self.test_message = test_message or DEFAULT_TEST_MESSAGE
        
        # æµ‹è¯•å‚æ•°
        self.min_concurrency = min_concurrency or DEFAULT_MIN_CONCURRENCY
        self.max_concurrency = max_concurrency or DEFAULT_MAX_CONCURRENCY
        self.step = step or DEFAULT_STEP
        self.test_rounds = test_rounds or DEFAULT_TEST_ROUNDS
        self.timeout = timeout or DEFAULT_TIMEOUT
        self.print_sample_errors = print_sample_errors or DEFAULT_PRINT_SAMPLE_ERRORS
        self.estimate_tokens_by_chars = estimate_tokens_by_chars or False
        self.chars_per_token = chars_per_token or DEFAULT_CHARS_PER_TOKEN

    def run_test(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•"""
        # æ£€æŸ¥ API Key
        if not self.api_key:
            print("âŒ é”™è¯¯ï¼šè¯·å…ˆè®¾ç½® API_KEY")
            print("æç¤ºï¼šåˆ›å»ºæµ‹è¯•å™¨æ—¶ä¼ å…¥ api_key å‚æ•°")
            return None
            
        print("ğŸš€ å¼€å§‹ API å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆSSE + TTFT + tokens/sï¼‰")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"API åœ°å€: {self.api_url}")
        print(f"æ¨¡å‹: {self.model}")
        print(f"æµ‹è¯•èŒƒå›´: {self.min_concurrency}-{self.max_concurrency} å¹¶å‘ (æ­¥é•¿: {self.step})")
        print(f"æ¯ä¸ªå¹¶å‘çº§åˆ«æµ‹è¯•è½®æ•°: {self.test_rounds}")
        print(f"å•è¯·æ±‚è¶…æ—¶: {self.timeout}ç§’")

        results = {}

        for concurrency in range(self.min_concurrency, self.max_concurrency + 5, self.step):
            result = test_concurrency(concurrency, self)
            results[concurrency] = result

            total_req = result.success_count + result.failure_count
            succ_rate = (result.success_count / total_req) if total_req else 0
            if succ_rate < 0.8:
                print(f"\nâš ï¸  æˆåŠŸç‡ä½äº 80%ï¼Œåœæ­¢ç»§ç»­æå‡å¹¶å‘")
                break

            time.sleep(2)  # é˜²æ­¢è¿‡åº¦å‹æµ‹

        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        self._print_summary(results)
        
        return results
        
    def _print_summary(self, results):
        """æ‰“å°æµ‹è¯•æ±‡æ€»æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print(f"ğŸ“‹ æµ‹è¯•æ±‡æ€»æŠ¥å‘Š {self.api_url}")
        print("=" * 60)
        print("\nå¹¶å‘çº§åˆ« | æˆåŠŸç‡ | å¹³å‡å®Œæˆæ—¶é—´ | å¹³å‡TTFT | å¹³å‡tokens/s")
        print("-" * 70)

        for concurrency, result in results.items():
            total_req = result.success_count + result.failure_count
            succ_rate = (result.success_count / total_req) * 100 if total_req else 0.0
            avg_time = statistics.mean(result.response_times) if result.response_times else float("nan")
            avg_ttft = statistics.mean(result.first_token_times) if result.first_token_times else float("nan")
            avg_tps = statistics.mean(result.tokens_per_sec) if result.tokens_per_sec else float("nan")
            print(f"{concurrency:8d} | {succ_rate:6.1f}% | {avg_time:10.2f}s | {avg_ttft:8.3f}s | {avg_tps:12.2f}")


class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    
    def __init__(self):
        self.success_count = 0
        self.failure_count = 0
        self.response_times = []      # æ•´ä½“å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰
        self.first_token_times = []   # TTFTï¼ˆç§’ï¼‰
        self.tokens_generated = []    # æ¯æ¬¡è¯·æ±‚çš„è¾“å‡º token æ•°
        self.tokens_per_sec = []      # æ¯æ¬¡è¯·æ±‚ tokens/s
        self.status_codes = []
        self.errors = []


def make_request(tester=None):
    """
    å‘é€å•ä¸ª SSE æµå¼è¯·æ±‚ï¼›ç»Ÿè®¡ï¼š
      - TTFTï¼ˆé¦– token æ—¶é—´ï¼‰
      - å®Œæˆæ—¶é—´ï¼ˆç§’ï¼‰
      - è¾“å‡º token æ•°ï¼ˆä¼˜å…ˆå– message_delta.usage.output_tokensï¼›å¦åˆ™å¯é€‰ä¼°ç®—ï¼‰
      - tokens/s = è¾“å‡º token æ•° / å®Œæˆæ—¶é—´
    
    Args:
        tester: APIPerformanceTester å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€å˜é‡
    """
    if tester is None:
        # ä¸æ”¯æŒæ—  tester çš„è°ƒç”¨æ–¹å¼ï¼Œå¿…é¡»ä¼ å…¥ tester
        raise ValueError("make_request å¿…é¡»ä¼ å…¥ tester å‚æ•°")
    else:
        # ä½¿ç”¨ tester å®ä¾‹çš„é…ç½®
        api_url = tester.api_url
        api_key = tester.api_key
        model = tester.model
        test_message = tester.test_message
        timeout = tester.timeout
        estimate_tokens_by_chars = tester.estimate_tokens_by_chars
        chars_per_token = tester.chars_per_token
        
    start_time = time.time()
    first_token_time = None
    output_tokens = None  # æ¥è‡ª message_delta çš„ usage.output_tokensï¼ˆç´¯è®¡ï¼‰
    approx_chars = 0      # å¦‚æœéœ€è¦ä¼°ç®—æ—¶ä½¿ç”¨

    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json",
    }

    payload = {
        "model": model,
        "max_tokens": 1024,
        "temperature": 0.2,
        "messages": [
            {"role": "user", "content": test_message}
        ],
        "stream": True
    }

    try:
        with requests.post(
            api_url,
            headers=headers,
            data=json.dumps(payload),
            stream=True,
            timeout=timeout,
        ) as r:
            status = r.status_code
            if status != 200:
                total_time = time.time() - start_time
                text = r.text[:200] if r.text else ""
                return (False, total_time, status, f"HTTP {status}: {text}", None, None, None)

            for raw_line in r.iter_lines(decode_unicode=True):
                if not raw_line or not raw_line.startswith("data:"):
                    continue

                chunk = raw_line[len("data:"):].strip()
                if not chunk:
                    continue

                try:
                    event = json.loads(chunk)
                except json.JSONDecodeError:
                    continue

                etype = event.get("type")

                # è®°å½•é¦– token æ—¶é—´ï¼ˆç¬¬ä¸€æ®µ text_delta å‡ºç°ï¼‰
                if etype == "content_block_delta":
                    delta = event.get("delta", {})
                    if delta.get("type") == "text_delta":
                        if first_token_time is None:
                            first_token_time = time.time() - start_time
                        if estimate_tokens_by_chars:
                            approx_chars += len(delta.get("text", ""))

                # usage ç´¯åŠ é€šå¸¸åœ¨ message_delta äº‹ä»¶é‡Œ
                if etype == "message_delta":
                    usage = event.get("usage") or {}
                    # ä¸€èˆ¬æ˜¯ç´¯è®¡å€¼ï¼ˆåˆ°å½“å‰ä¸ºæ­¢çš„è¾“å‡º token æ•°ï¼‰
                    if "output_tokens" in usage:
                        output_tokens = usage.get("output_tokens")

                if etype == "message_stop":
                    total_time = time.time() - start_time
                    if first_token_time is None:
                        first_token_time = total_time  # æç«¯æƒ…å†µï¼šå‡ ä¹æ— è¾“å‡º

                    # è‹¥æœªæ‹¿åˆ° usage.output_tokensï¼ŒæŒ‰éœ€ä¼°ç®—
                    if output_tokens is None and estimate_tokens_by_chars:
                        output_tokens = max(1, int(approx_chars / chars_per_token))

                    # è®¡ç®— tokens/s
                    tokens_per_sec = None
                    if output_tokens is not None and total_time > 0:
                        tokens_per_sec = output_tokens / total_time

                    return (True, total_time, status, None, first_token_time, output_tokens, tokens_per_sec)

            # æœªæ”¶åˆ° message_stop
            total_time = time.time() - start_time
            return (False, total_time, status, "Stream ended without message_stop", first_token_time, output_tokens, None)

    except Exception as e:
        total_time = time.time() - start_time
        return (False, total_time, None, str(e), first_token_time, output_tokens, None)


def test_concurrency(concurrency_level, tester=None):
    """æµ‹è¯•æŒ‡å®šå¹¶å‘çº§åˆ«ï¼ˆSSE + TTFT + tokens/sï¼‰
    
    Args:
        concurrency_level: å¹¶å‘çº§åˆ«
        tester: APIPerformanceTester å®ä¾‹ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å…¨å±€é…ç½®
    """
    print(f"\nğŸ”„ æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency_level}")
    print("=" * 50)

    result = TestResult()

    test_rounds = tester.test_rounds
    
    for round_num in range(test_rounds):
        print(f"   ç¬¬ {round_num + 1}/{test_rounds} è½®æµ‹è¯•...")

        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency_level) as executor:
            futures = [executor.submit(make_request, tester) for _ in range(concurrency_level)]

            for future in concurrent.futures.as_completed(futures):
                success, total_time, status_code, error, ttft, out_tokens, tps = future.result()

                if success:
                    result.success_count += 1
                else:
                    result.failure_count += 1
                    if error:
                        result.errors.append(error)

                result.response_times.append(total_time)
                if ttft is not None:
                    result.first_token_times.append(ttft)
                if out_tokens is not None:
                    result.tokens_generated.append(out_tokens)
                if tps is not None:
                    result.tokens_per_sec.append(tps)
                if status_code:
                    result.status_codes.append(status_code)

    # ç»Ÿè®¡
    total_requests = result.success_count + result.failure_count
    success_rate = (result.success_count / total_requests) * 100 if total_requests else 0.0

    def safe_mean(xs): return statistics.mean(xs) if xs else float("nan")
    def safe_min(xs):  return min(xs) if xs else float("nan")
    def safe_max(xs):  return max(xs) if xs else float("nan")

    def percentile(xs, p):
        if not xs:
            return float("nan")
        xs_sorted = sorted(xs)
        k = (len(xs_sorted) - 1) * p
        f = int(k)
        c = min(f + 1, len(xs_sorted) - 1)
        if f == c:
            return xs_sorted[f]
        return xs_sorted[f] + (xs_sorted[c] - xs_sorted[f]) * (k - f)

    avg_response_time = safe_mean(result.response_times)
    min_response_time = safe_min(result.response_times)
    max_response_time = safe_max(result.response_times)

    avg_ttft = safe_mean(result.first_token_times)
    p50_ttft = percentile(result.first_token_times, 0.5)
    p95_ttft = percentile(result.first_token_times, 0.95)

    avg_tokens = safe_mean(result.tokens_generated)
    sum_tokens = sum(result.tokens_generated) if result.tokens_generated else 0
    avg_tps = safe_mean(result.tokens_per_sec)
    p50_tps = percentile(result.tokens_per_sec, 0.5)
    p95_tps = percentile(result.tokens_per_sec, 0.95)
    max_tps = safe_max(result.tokens_per_sec)

    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è¯·æ±‚æ•°: {total_requests}")
    print(f"   æˆåŠŸ: {result.success_count} | å¤±è´¥: {result.failure_count}")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"   å¹³å‡å®Œæˆæ—¶é—´: {avg_response_time:.2f}s  (æœ€å¿« {min_response_time:.2f}s | æœ€æ…¢ {max_response_time:.2f}s)")
    print(f"   TTFT(é¦–å­—å“åº”): å¹³å‡ {avg_ttft:.3f}s | P50 {p50_ttft:.3f}s | P95 {p95_ttft:.3f}s")
    if result.tokens_per_sec:
        print(f"   è¾“å‡ºToken: æ€»è®¡ {sum_tokens} | å•æ¬¡å¹³å‡ {avg_tokens:.1f}")
        print(f"   è¾“å‡ºé€Ÿç‡(tokens/s): å¹³å‡ {avg_tps:.2f} | P50 {p50_tps:.2f} | P95 {p95_tps:.2f} | æœ€é«˜ {max_tps:.2f}")
    else:
        print("   âš ï¸ æœªè·å–åˆ° usage.output_tokensï¼›å¦‚éœ€ä¼°ç®—ï¼Œè¯·ä½¿ç”¨ --estimate-tokens å‚æ•°ã€‚")

    if result.errors:
        print_limit = tester.print_sample_errors
        print(f"\nâŒ é”™è¯¯æ±‡æ€» (å‰{print_limit}ä¸ª):")
        for i, error in enumerate(result.errors[:print_limit], 1):
            print(f"   {i}. {error}")

    return result


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="API æ€§èƒ½æµ‹è¯•å·¥å…· - æµ‹è¯• BigModel API çš„å¹¶å‘æ€§èƒ½",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python api_performance_tester.py --key your_api_key_here
  python api_performance_tester.py --key your_api_key_here --model glm-4-0528
  python api_performance_tester.py --key your_api_key_here --min 5 --max 50 --step 5
  python api_performance_tester.py --key your_api_key_here --rounds 3 --timeout 60
        """
    )
    
    # API é…ç½®å‚æ•°
    parser.add_argument(
        "--key", 
        required=True,
        help="API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰"
    )
    parser.add_argument(
        "--url",
        default=DEFAULT_API_URL,
        help="API æ¥å£åœ°å€ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    parser.add_argument(
        "--message",
        default=DEFAULT_TEST_MESSAGE,
        help="æµ‹è¯•æ¶ˆæ¯å†…å®¹"
    )
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument(
        "--min",
        type=int,
        default=DEFAULT_MIN_CONCURRENCY,
        help="æœ€å°å¹¶å‘çº§åˆ«ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--max",
        type=int,
        default=DEFAULT_MAX_CONCURRENCY,
        help="æœ€å¤§å¹¶å‘çº§åˆ«ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--step",
        type=int,
        default=DEFAULT_STEP,
        help="å¹¶å‘çº§åˆ«æ­¥é•¿ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--rounds",
        type=int,
        default=DEFAULT_TEST_ROUNDS,
        help="æ¯ä¸ªå¹¶å‘çº§åˆ«æµ‹è¯•è½®æ•°ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=DEFAULT_TIMEOUT,
        help="å•è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--estimate-tokens",
        action="store_true",
        help="ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®— tokensï¼ˆé»˜è®¤ï¼šä¸å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--chars-per-token",
        type=float,
        default=DEFAULT_CHARS_PER_TOKEN,
        help="æ¯ä¸ª token çš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼š%(default).1fï¼‰"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = APIPerformanceTester(
        api_url=args.url,
        api_key=args.key,
        model=args.model,
        test_message=args.message,
        min_concurrency=args.min,
        max_concurrency=args.max,
        step=args.step,
        test_rounds=args.rounds,
        timeout=args.timeout,
        estimate_tokens_by_chars=args.estimate_tokens,
        chars_per_token=args.chars_per_token
    )
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_test()
    
    return results


if __name__ == "__main__":
    main()