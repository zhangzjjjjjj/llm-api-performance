"""
ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æµ‹è¯•å·¥å…·

ç”¨äºæµ‹è¯• LLM æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ï¼Œæ”¯æŒå¤šç§é¢„è®¾å¤§å°ï¼ˆ32k, 64k, 128kï¼‰å’Œè‡ªå®šä¹‰é•¿åº¦ã€‚
é€šè¿‡é€’å¢æµ‹è¯•æ³•ç¡®å®šæ¨¡å‹å®é™…æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚

ä½œè€…: Claude
ç‰ˆæœ¬: 1.0.0
"""

import argparse
import requests
import time
from datetime import datetime
import statistics
import json
import random
import os
from typing import List, Dict, Optional, Tuple, Any
import dataclasses

try:
    from tokenizers import Tokenizer
    TOKENIZERS_AVAILABLE = True
except ImportError:
    TOKENIZERS_AVAILABLE = False
    print("âš ï¸ è­¦å‘Šï¼štokenizers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ã€‚å»ºè®®è¿è¡Œ: pip install tokenizers")


@dataclasses.dataclass
class ContextTestResult:
    """ä¸Šä¸‹æ–‡æµ‹è¯•ç»“æœ"""
    success: bool
    success_count: int
    failure_count: int
    avg_response_time: float
    errors: List[str]
    actual_tokens: Optional[int] = None
    input_tokens: Optional[int] = None
    is_compressed: bool = False
    compression_ratio: Optional[float] = None


# é»˜è®¤é…ç½®å€¼
DEFAULT_API_URL = "https://open.bigmodel.cn/api/anthropic/v1/messages"
DEFAULT_CHAT_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
DEFAULT_GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/{model}:streamGenerateContent"
DEFAULT_MODEL = "glm-4.5"
DEFAULT_TEST_ROUNDS = 1
DEFAULT_TIMEOUT = 300
DEFAULT_CHARS_PER_TOKEN_EN = 4.0  # è‹±æ–‡é»˜è®¤4å­—ç¬¦/token
DEFAULT_CHARS_PER_TOKEN_CN = 2.0  # ä¸­æ–‡é»˜è®¤2å­—ç¬¦/token
DEFAULT_MAX_PARAGRAPHS = 100000

# æ¨¡å‹åˆ°åˆ†è¯å™¨æ–‡ä»¶çš„æ˜ å°„
MODEL_TOKENIZER_MAP = {
    "glm": "tokenizer_glm.json",          # GLMç³»åˆ—æ¨¡å‹
    "glm-4": "tokenizer_glm.json",        # GLM-4ç³»åˆ—
    "glm-4.5": "tokenizer_glm.json",      # GLM-4.5ç³»åˆ—  
    "deepseek": "tokenizer_ds.json",      # DeepSeekç³»åˆ—
    "deepseek-chat": "tokenizer_ds.json", # DeepSeek Chat
    "deepseek-coder": "tokenizer_ds.json",# DeepSeek Coder
    "gemini": "tokenizer_glm.json",       # Geminiç³»åˆ—æ¨¡å‹ï¼ˆæš‚ç”¨GLMåˆ†è¯å™¨ï¼‰
    "gemini-pro": "tokenizer_glm.json",   # Gemini Pro
    "gemini-1.5-pro": "tokenizer_glm.json", # Gemini 1.5 Pro
    "gemini-1.5-flash": "tokenizer_glm.json" # Gemini 1.5 Flash
}

# é¢„è®¾çš„ä¸Šä¸‹æ–‡å¤§å°ï¼ˆtokensï¼‰
PRESET_SIZES = {
    "1k": 1024*1,
    "2k": 1024*2,
    "4k": 1024*4,
    "8k": 1024*8,
    "16k": 1024*16,
    "32k": 1024*32,
    "64k": 1024*64,
    "128k": 1024*128,
    "256k": 1024*256,
    "512k": 1024*512
}


class ContextLengthTester:
    """ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æµ‹è¯•å·¥å…·"""
    
    def __init__(self, api_url: Optional[str] = None, api_key: Optional[str] = None, 
                 model: Optional[str] = None, test_sizes: Optional[List[str]] = None,
                 test_rounds: Optional[int] = None, timeout: Optional[int] = None, 
                 use_chat_api: Optional[bool] = None, chars_per_token: Optional[float] = None, 
                 output_file: Optional[str] = None, max_paragraphs: Optional[int] = None,
                 use_english: Optional[bool] = None, 
                 disable_thinking: Optional[bool] = None,
                 show_detail: Optional[bool] = None,
                 query_num: Optional[int] = None,
                 use_gemini_api: Optional[bool] = None):
        """åˆå§‹åŒ–æµ‹è¯•é…ç½®
        
        Args:
            api_url: API åœ°å€
            api_key: API å¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹
            test_sizes: æµ‹è¯•å¤§å°åˆ—è¡¨ï¼ˆå¦‚ ["32k", "64k", "192k"]ï¼‰
            test_rounds: æµ‹è¯•è½®æ•°
            timeout: è¶…æ—¶æ—¶é—´
            use_chat_api: æ˜¯å¦ä½¿ç”¨ Chat API æ¥å£
            chars_per_token: å­—ç¬¦/token æ¯”ç‡
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_paragraphs: æœ€å¤§æ®µè½æ•°é‡é™åˆ¶
            use_english: æ˜¯å¦ä½¿ç”¨è‹±æ–‡ç”Ÿæˆpromptï¼ˆé»˜è®¤ä½¿ç”¨ä¸­æ–‡ï¼‰
            disable_thinking: æ˜¯å¦ç¦ç”¨ GLM æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰
            show_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†çš„å“åº”å†…å®¹å’Œpayloadï¼ˆé»˜è®¤ä¸æ˜¾ç¤ºï¼‰
            query_num: æ’å…¥çš„éšæœºæ•°æ•°é‡ï¼ˆé»˜è®¤ä¸º1ï¼‰
            use_gemini_api: æ˜¯å¦ä½¿ç”¨ Gemini API æ¥å£
        """
        # API é…ç½®
        self.use_chat_api = use_chat_api or False
        self.use_gemini_api = use_gemini_api or False
        if self.use_gemini_api:
            self.api_url = DEFAULT_GEMINI_API_URL.format(model=model or DEFAULT_MODEL)
        elif self.use_chat_api and api_url is None:
            self.api_url = DEFAULT_CHAT_API_URL
        else:
            self.api_url = api_url or DEFAULT_API_URL
        self.api_key = api_key
        self.model = model or DEFAULT_MODEL
        
        # æµ‹è¯•å‚æ•°
        self.test_sizes = test_sizes or []
        self.test_rounds = test_rounds or DEFAULT_TEST_ROUNDS
        self.timeout = timeout or DEFAULT_TIMEOUT
        self.use_english = use_english or False
        self.disable_thinking = disable_thinking
        self.show_detail = show_detail or False
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆé€‚çš„å­—ç¬¦/tokenæ¯”ç‡
        if chars_per_token is not None:
            self.chars_per_token = chars_per_token
        else:
            self.chars_per_token = DEFAULT_CHARS_PER_TOKEN_EN if self.use_english else DEFAULT_CHARS_PER_TOKEN_CN
            
        self.max_paragraphs = max_paragraphs or DEFAULT_MAX_PARAGRAPHS
        self.query_num = query_num or 1
        
        # åˆ†è¯å™¨é…ç½® - é»˜è®¤å°è¯•ä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨
        self.use_tokenizer = TOKENIZERS_AVAILABLE
        self.tokenizer = None
        if self.use_tokenizer:
            try:
                # æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨æ–‡ä»¶
                tokenizer_file = self._get_tokenizer_file()
                if tokenizer_file and os.path.exists(tokenizer_file):
                    self.tokenizer = Tokenizer.from_file(tokenizer_file)
                    print(f"âœ… ä½¿ç”¨æœ¬åœ°åˆ†è¯å™¨: {tokenizer_file}")
                else:
                    print(f"âš ï¸ åˆ†è¯å™¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå›é€€åˆ°å­—ç¬¦ä¼°ç®—: {tokenizer_file}")
                    self.use_tokenizer = False
            except Exception as e:
                print(f"âš ï¸ åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦ä¼°ç®—: {e}")
                self.use_tokenizer = False
        else:
            print(f"ğŸ“ ä½¿ç”¨å­—ç¬¦ä¼°ç®—æ¨¡å¼ï¼ˆ{self.chars_per_token} å­—ç¬¦/tokenï¼‰")
        
        # ç”Ÿæˆæµ‹è¯•å¤§å°åˆ—è¡¨
        self.test_tokens_list: List[int] = self._generate_test_sizes()
        
        # è¾“å‡ºæ–‡ä»¶é…ç½®
        self.output_file = output_file
        
    def _generate_test_sizes(self) -> List[int]:
        """ç”Ÿæˆè¦æµ‹è¯•çš„ tokens å¤§å°åˆ—è¡¨"""
        sizes = []
        
        # å¤„ç†æ‰€æœ‰æŒ‡å®šçš„å¤§å°
        for size_name in self.test_sizes:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é¢„è®¾å¤§å°
            if size_name in PRESET_SIZES:
                sizes.append(PRESET_SIZES[size_name])
            else:
                # å°è¯•è§£æä¸ºè‡ªå®šä¹‰å¤§å°
                try:
                    custom_size = parse_custom_size(size_name)
                    sizes.append(custom_size)
                except ValueError as e:
                    print(f"âŒ è­¦å‘Šï¼šå¿½ç•¥æ— æ•ˆçš„å¤§å° '{size_name}': {e}")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å¤§å°ï¼Œä½¿ç”¨é»˜è®¤çš„æ¸è¿›å¼æµ‹è¯•
        if not sizes:
            # ä»å°åˆ°å¤§ï¼Œé€æ­¥å¢åŠ ç›´åˆ°æ‰¾åˆ°æœ€å¤§å€¼
            sizes = [1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000, 256000, 512000]
        
        return sorted(sizes)
    
    def _get_size_name(self, target_tokens: int) -> str:
        """æ ¹æ®tokenæ•°é‡è·å–ç”¨æˆ·è¾“å…¥çš„å¤§å°åç§°"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯é¢„è®¾å¤§å°
        for size_name, tokens in PRESET_SIZES.items():
            if tokens == target_tokens:
                return size_name
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·è¾“å…¥çš„è‡ªå®šä¹‰å¤§å°
        for size_str in self.test_sizes:
            try:
                custom_tokens = parse_custom_size(size_str)
                if custom_tokens == target_tokens:
                    return size_str
            except ValueError:
                continue
        
        # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æœ€æ¥è¿‘çš„kè¡¨ç¤º
        if target_tokens >= 1024:
            k_value = target_tokens / 1024
            if k_value.is_integer():
                return f"{int(k_value)}k"
            else:
                return f"{target_tokens}"
        else:
            return f"{target_tokens}"
    
    def _get_tokenizer_file(self) -> Optional[str]:
        """æ ¹æ®æ¨¡å‹åç§°è·å–å¯¹åº”çš„åˆ†è¯å™¨æ–‡ä»¶è·¯å¾„"""
        # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æ£€æŸ¥æ¨¡å‹åç§°ï¼ˆè½¬ä¸ºå°å†™è¿›è¡ŒåŒ¹é…ï¼‰
        model_lower = self.model.lower()
        
        # éå†æ˜ å°„è¡¨ï¼Œæ‰¾åˆ°åŒ¹é…çš„åˆ†è¯å™¨
        for model_key, tokenizer_filename in MODEL_TOKENIZER_MAP.items():
            if model_key in model_lower:
                tokenizer_path = os.path.join(current_dir, tokenizer_filename)
                return tokenizer_path
        
        # é»˜è®¤ä½¿ç”¨GLMåˆ†è¯å™¨ï¼ˆå¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ï¼‰
        default_tokenizer = os.path.join(current_dir, "tokenizer_glm.json")
        print(f"âš ï¸ æ¨¡å‹ {self.model} æœªæ‰¾åˆ°å¯¹åº”åˆ†è¯å™¨ï¼Œä½¿ç”¨é»˜è®¤GLMåˆ†è¯å™¨")
        return default_tokenizer
    
    def _count_tokens_all_tokenizers(self, text: str) -> Dict[str, int]:
        """ä½¿ç”¨æ‰€æœ‰å¯ç”¨åˆ†è¯å™¨ç»Ÿè®¡tokenæ•°é‡"""
        results = {}
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # æµ‹è¯•æ‰€æœ‰åˆ†è¯å™¨
        for tokenizer_name, tokenizer_file in [("GLM", "tokenizer_glm.json"), ("DeepSeek", "tokenizer_ds.json")]:
            tokenizer_path = os.path.join(current_dir, tokenizer_file)
            
            if os.path.exists(tokenizer_path) and TOKENIZERS_AVAILABLE:
                try:
                    tokenizer = Tokenizer.from_file(tokenizer_path)
                    encoding = tokenizer.encode(text)
                    results[tokenizer_name] = len(encoding.ids)
                except Exception:
                    results[tokenizer_name] = None
            else:
                results[tokenizer_name] = None
        
        # æ·»åŠ å­—ç¬¦ä¼°ç®—ç»“æœ
        results["å­—ç¬¦ä¼°ç®—(EN)"] = int(len(text) / DEFAULT_CHARS_PER_TOKEN_EN)
        results["å­—ç¬¦ä¼°ç®—(CN)"] = int(len(text) / DEFAULT_CHARS_PER_TOKEN_CN)
        
        return results
    
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        if self.use_tokenizer and self.tokenizer:
            try:
                encoding = self.tokenizer.encode(text)
                return len(encoding.ids)
            except Exception as e:
                print(f"âš ï¸ åˆ†è¯å™¨ç¼–ç å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦ä¼°ç®—: {e}")
                # å›é€€åˆ°å­—ç¬¦ä¼°ç®—
                return int(len(text) / self.chars_per_token)
        else:
            # å›é€€åˆ°å­—ç¬¦ä¼°ç®—
            return int(len(text) / self.chars_per_token)
    
    def _safe_truncate_text(self, text: str, target_tokens: int) -> Optional[str]:
        """å®‰å…¨åœ°æˆªæ–­æ–‡æœ¬åˆ°ç›®æ ‡tokenæ•°"""
        if not self.use_tokenizer or not self.tokenizer:
            return None
            
        try:
            encoding = self.tokenizer.encode(text)
            if len(encoding.ids) <= target_tokens:
                return text
                
            # æˆªæ–­åˆ°ç›®æ ‡tokenæ•°
            truncated_encoding = encoding.truncate(target_tokens)
            if truncated_encoding and hasattr(truncated_encoding, 'ids') and truncated_encoding.ids:
                return self.tokenizer.decode(truncated_encoding.ids)
            return None
        except Exception:
            return None
    
    def _fine_tune_content_length(self, content: str, target_tokens: int) -> str:
        """ç²¾ç»†è°ƒæ•´å†…å®¹é•¿åº¦ä»¥ç²¾ç¡®åŒ¹é…ç›®æ ‡tokens"""
        if not self.use_tokenizer or not self.tokenizer:
            return content
        
        # æ‰¾åˆ°å†…å®¹éƒ¨åˆ†çš„å¼€å§‹å’Œç»“æŸ
        start_marker_end = content.find("\n\n") + 2
        if start_marker_end < 10:
            start_marker_end = content.find("[START]") + 50
        
        end_marker_start = content.rfind("[END]")
        if end_marker_start == -1:
            return content
        
        # ä¿æŠ¤å¼€å§‹å’Œç»“æŸæ ‡è®°
        protected_start = content[:start_marker_end]
        protected_end = content[end_marker_start:]
        content_part = content[start_marker_end:end_marker_start]
        
        try:
            # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°æœ€ä½³é•¿åº¦
            content_encoding = self.tokenizer.encode(content_part)
            target_content_tokens = target_tokens - self._count_tokens(protected_start + protected_end)
            
            if target_content_tokens <= 0:
                return content
            
            # äºŒåˆ†æŸ¥æ‰¾
            left, right = 0, len(content_encoding.ids)
            best_encoding = content_encoding
            
            while left < right:
                mid = (left + right + 1) // 2
                test_encoding = content_encoding.truncate(mid)
                if test_encoding and hasattr(test_encoding, 'ids') and len(test_encoding.ids) <= target_content_tokens:
                    best_encoding = test_encoding
                    left = mid
                else:
                    right = mid - 1
            
            # é‡å»ºå†…å®¹
            fine_tuned_content = protected_start + self.tokenizer.decode(best_encoding.ids) + protected_end
            return fine_tuned_content
        except Exception:
            return content
    
    def _generate_test_content(self, target_tokens: int) -> Tuple[str, List[int]]:
        """ç”ŸæˆæŒ‡å®š token æ•°é‡çš„æµ‹è¯•å†…å®¹
        
        ä½¿ç”¨å¤šæ ·åŒ–çš„æ–‡æœ¬æ¨¡å¼ï¼Œé¿å…é‡å¤å†…å®¹è§¦å‘è¿‡æ»¤
        åœ¨promptä¸­éšæœºåˆ†å¸ƒæŒ‡å®šæ•°é‡çš„éšæœºæ•°ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸæ­£å¤„ç†å®Œæ•´ä¸Šä¸‹æ–‡
        
        Returns:
            Tuple[str, List[int]]: (æµ‹è¯•å†…å®¹, éšæœºæ•°åˆ—è¡¨)
        """
        try:
            # ç”ŸæˆæŒ‡å®šæ•°é‡çš„éšæœºæ•°
            random_numbers = [random.randint(100, 999) for _ in range(self.query_num)]
            
            # è·å–æ ‡è®°ï¼ˆåŒ…å«éšæœºæ•°æç¤ºï¼‰
            start_marker = f"[START] Context length test begins. The target token count is: {target_tokens}.\n\n"
            end_marker = f"\n\n[END] Context length test ends. Please find all random numbers hidden in the text and list them. Tell me how many random numbers you found and what each number is."
            
            # è®¡ç®—æ ‡è®°å ç”¨çš„ tokens
            marker_tokens = self._count_tokens(start_marker + end_marker)
            
            # è®¡ç®—å¯ç”¨äºåŸºç¡€æ–‡æœ¬çš„ tokens
            available_tokens = target_tokens - marker_tokens
            
            # ä½¿ç”¨å˜åŒ–çš„å†…å®¹è€Œä¸æ˜¯é‡å¤ç›¸åŒå†…å®¹
            content_parts = []
            current_tokens = 0
            paragraph_num = 0
            
            # è®¡ç®—æ¯ä¸ªå†…å®¹éƒ¨åˆ†çš„å¹³å‡å¤§å°
            random_number_text_tokens = self._count_tokens("\n[random num: 123]\n\n")
            # æ›´å‡†ç¡®åœ°è®¡ç®—å¯ç”¨å†…å®¹tokens
            if self.query_num > 0:
                total_content_tokens = available_tokens - (self.query_num * random_number_text_tokens)
            else:
                total_content_tokens = available_tokens
            
            # æ›´ç²¾ç¡®çš„å†…å®¹ç”Ÿæˆç­–ç•¥
            if self.query_num == 1:
                # å•ä¸ªéšæœºæ•°çš„æƒ…å†µï¼šç²¾ç¡®è®¡ç®—æ¯éƒ¨åˆ†å¤§å°
                # è®¡ç®—æ ‡è®°çš„å®é™…tokenæ•°
                actual_start_tokens = self._count_tokens(start_marker)
                actual_end_tokens = self._count_tokens(end_marker)
                actual_random_tokens = self._count_tokens("\n[random num: 123]\n\n")
                
                # é‡æ–°è®¡ç®—å¯ç”¨tokens
                total_available = target_tokens - actual_start_tokens - actual_end_tokens - actual_random_tokens
                
                # ç¬¬ä¸€éƒ¨åˆ†å 60%ï¼Œç¬¬äºŒéƒ¨åˆ†å 40%
                first_part_target = total_available * 0.6
                second_part_target = total_available * 0.4
                
                # ç”Ÿæˆç¬¬ä¸€éƒ¨åˆ†
                part_tokens = 0
                while part_tokens < first_part_target * 0.99 and paragraph_num < self.max_paragraphs:
                    # è®¡ç®—è¿˜éœ€è¦å¤šå°‘tokens
                    needed = first_part_target - part_tokens
                    if needed < 50:
                        break
                        
                    paragraph_text = self._get_alternative_base_text(paragraph_num)
                    paragraph_tokens = self._count_tokens(paragraph_text)
                    
                    if paragraph_tokens <= needed * 1.1:  # å…è®¸ç¨å¾®è¶…å‡ºï¼Œåé¢ä¼šè°ƒæ•´
                        content_parts.append(paragraph_text)
                        part_tokens += paragraph_tokens
                        current_tokens += paragraph_tokens
                        paragraph_num += 1
                    else:
                        # å¦‚æœæ®µè½å¤ªå¤§ï¼Œå°è¯•åˆ†å‰²
                        if self.use_tokenizer and self.tokenizer and needed > 100:
                            # ä½¿ç”¨å®‰å…¨çš„æ–¹æ³•åˆ†å‰²æ–‡æœ¬
                            partial_text = self._safe_truncate_text(paragraph_text, int(needed))
                            if partial_text:
                                partial_tokens = self._count_tokens(partial_text)
                                if partial_tokens > 0:
                                    content_parts.append(partial_text)
                                    part_tokens += partial_tokens
                                    current_tokens += partial_tokens
                        break
                
                # æ’å…¥éšæœºæ•°
                hidden_number_text = f"\n[random num: {random_numbers[0]}]\n\n"
                content_parts.append(hidden_number_text)
                current_tokens += self._count_tokens(hidden_number_text)
                
                # ç¬¬äºŒéƒ¨åˆ†å†…å®¹
                part_tokens = 0
                while part_tokens < second_part_target * 0.99 and paragraph_num < self.max_paragraphs:
                    needed = second_part_target - part_tokens
                    if needed < 50:
                        break
                        
                    paragraph_text = self._get_alternative_base_text(paragraph_num)
                    paragraph_tokens = self._count_tokens(paragraph_text)
                    
                    if paragraph_tokens <= needed * 1.1:
                        content_parts.append(paragraph_text)
                        part_tokens += paragraph_tokens
                        current_tokens += paragraph_tokens
                        paragraph_num += 1
                    else:
                        # å¦‚æœæ®µè½å¤ªå¤§ï¼Œå°è¯•åˆ†å‰²
                        if self.use_tokenizer and self.tokenizer and needed > 100:
                            # ä½¿ç”¨å®‰å…¨çš„æ–¹æ³•åˆ†å‰²æ–‡æœ¬
                            partial_text = self._safe_truncate_text(paragraph_text, int(needed))
                            if partial_text:
                                partial_tokens = self._count_tokens(partial_text)
                                if partial_tokens > 0:
                                    content_parts.append(partial_text)
                                    part_tokens += partial_tokens
                                    current_tokens += partial_tokens
                        break
            else:
                # å¤šä¸ªéšæœºæ•°çš„æƒ…å†µï¼šç²¾ç¡®è®¡ç®—æ¯éƒ¨åˆ†
                # è®¡ç®—å®é™…çš„æ ‡è®°tokenæ•°
                actual_start_tokens = self._count_tokens(start_marker)
                actual_end_tokens = self._count_tokens(end_marker)
                actual_random_tokens = self._count_tokens("\n[random num: 123]\n\n")
                
                # é‡æ–°è®¡ç®—å¯ç”¨tokens
                total_available = target_tokens - actual_start_tokens - actual_end_tokens - (self.query_num * actual_random_tokens)
                content_sections = self.query_num + 1  # æ¯”éšæœºæ•°å¤šä¸€ä¸ªéƒ¨åˆ†
                tokens_per_section = total_available / content_sections
                
                for i in range(self.query_num):
                    # å½“å‰éƒ¨åˆ†çš„å†…å®¹ï¼ˆé™¤äº†æœ€åä¸€ä¸ªéšæœºæ•°åä¸éœ€è¦å†…å®¹ï¼‰
                    if i < self.query_num:
                        section_target = tokens_per_section
                        section_tokens = 0
                        
                        while section_tokens < section_target * 0.99 and paragraph_num < self.max_paragraphs:
                            needed = section_target - section_tokens
                            if needed < 30:  # å¯¹äºå¤šä¸ªéšæœºæ•°ï¼Œå¯ä»¥æ¥å—æ›´å°çš„å‰©ä½™
                                break
                                
                            paragraph_text = self._get_alternative_base_text(paragraph_num)
                            paragraph_tokens = self._count_tokens(paragraph_text)
                            
                            if paragraph_tokens <= needed * 1.1:
                                content_parts.append(paragraph_text)
                                section_tokens += paragraph_tokens
                                current_tokens += paragraph_tokens
                                paragraph_num += 1
                            else:
                                # å¦‚æœæ®µè½å¤ªå¤§ï¼Œå°è¯•åˆ†å‰²
                                if self.use_tokenizer and self.tokenizer and needed > 50:
                                    # ä½¿ç”¨å®‰å…¨çš„æ–¹æ³•åˆ†å‰²æ–‡æœ¬
                                    partial_text = self._safe_truncate_text(paragraph_text, int(needed))
                                    if partial_text:
                                        partial_tokens = self._count_tokens(partial_text)
                                        if partial_tokens > 0:
                                            content_parts.append(partial_text)
                                            section_tokens += partial_tokens
                                            current_tokens += partial_tokens
                                break
                    
                    # æ’å…¥éšæœºæ•°
                    hidden_number_text = f"\n[random num: {random_numbers[i]}]\n\n"
                    content_parts.append(hidden_number_text)
                    current_tokens += self._count_tokens(hidden_number_text)
            
            # ç»„åˆå®Œæ•´å†…å®¹
            full_content = start_marker + "".join(content_parts) + end_marker
            
            # è°ƒè¯•ä¿¡æ¯
            if self.show_detail:
                actual_content_tokens = current_tokens - (self.query_num * self._count_tokens("\n[random num: 123]\n\n"))
                print(f"   [DEBUG] ç”Ÿæˆå†…å®¹tokens: {actual_content_tokens:,}, æ€»è®¡tokens: {current_tokens:,}")
            
            # å¦‚æœä½¿ç”¨çš„tokenså¤ªå°‘ï¼Œæ·»åŠ æ›´å¤šå†…å®¹
            if current_tokens < available_tokens * 0.7:
                # è®¡ç®—è¿˜éœ€è¦å¤šå°‘tokens
                remaining_tokens = available_tokens - current_tokens
                # åœ¨æœ€åä¸€ä¸ªéšæœºæ•°å‰æ·»åŠ å†…å®¹
                if content_parts:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªéšæœºæ•°çš„ä½ç½®
                    for i in range(len(content_parts) - 1, -1, -1):
                        if "[random num:" in content_parts[i]:
                            # åœ¨è¿™ä¸ªä½ç½®ä¹‹å‰æ’å…¥æ›´å¤šå†…å®¹
                            additional_content = []
                            add_tokens = 0
                            while add_tokens < remaining_tokens * 0.8 and paragraph_num < self.max_paragraphs:
                                paragraph_text = self._get_alternative_base_text(paragraph_num)
                                paragraph_tokens = self._count_tokens(paragraph_text)
                                
                                if add_tokens + paragraph_tokens <= remaining_tokens:
                                    additional_content.append(paragraph_text)
                                    add_tokens += paragraph_tokens
                                    paragraph_num += 1
                                else:
                                    break
                            
                            # æ’å…¥é¢å¤–å†…å®¹
                            if additional_content:
                                content_parts[i:i] = additional_content
                                current_tokens += add_tokens
                                if self.show_detail:
                                    print(f"   [DEBUG] æ·»åŠ é¢å¤–å†…å®¹tokens: {add_tokens:,}")
                            break
            
            # è°ƒæ•´å†…å®¹é•¿åº¦ä»¥åŒ¹é…ç›®æ ‡ tokens
            adjusted_content = self._adjust_content_length(full_content, target_tokens)
            
            # æœ€ç»ˆæ£€æŸ¥å’Œå¾®è°ƒ
            final_tokens = self._count_tokens(adjusted_content)
            error_rate = abs(final_tokens - target_tokens) / target_tokens * 100
            
            if self.show_detail:
                print(f"   [DEBUG] è°ƒæ•´å‰tokens: {current_tokens:,}, è°ƒæ•´åtokens: {final_tokens:,}, ç›®æ ‡: {target_tokens:,}")
                print(f"   [DEBUG] è¯¯å·®ç‡: {error_rate:.2f}%")
            
            # å¦‚æœè¯¯å·®ä»ç„¶å¤ªå¤§ï¼ˆ>5%ï¼‰ï¼Œå°è¯•è¿›ä¸€æ­¥è°ƒæ•´
            if error_rate > 5 and self.use_tokenizer and self.tokenizer:
                if self.show_detail:
                    print(f"   [DEBUG] è¯¯å·®ç‡è¿‡å¤§ï¼Œè¿›è¡Œç²¾ç»†è°ƒæ•´...")
                
                # è®¡ç®—éœ€è¦æ·»åŠ æˆ–åˆ é™¤çš„tokens
                token_diff = target_tokens - final_tokens
                
                if token_diff > 0:
                    # éœ€è¦æ·»åŠ æ›´å¤šå†…å®¹
                    additional_content = []
                    additional_tokens = 0
                    
                    while additional_tokens < token_diff and paragraph_num < self.max_paragraphs:
                        # ç”Ÿæˆä¸€ä¸ªå°æ®µè½
                        paragraph_text = self._get_alternative_base_text(paragraph_num)
                        paragraph_tokens = self._count_tokens(paragraph_text)
                        
                        if additional_tokens + paragraph_tokens <= token_diff:
                            additional_content.append(paragraph_text)
                            additional_tokens += paragraph_tokens
                            paragraph_num += 1
                        else:
                            # ä½¿ç”¨åˆ†è¯å™¨ç²¾ç¡®åˆ†å‰²
                            remaining = token_diff - additional_tokens
                            if remaining > 10:
                                partial_text = self._safe_truncate_text(paragraph_text, int(remaining))
                                if partial_text:
                                    partial_tokens = self._count_tokens(partial_text)
                                    if partial_tokens > 0:
                                        additional_content.append(partial_text)
                                        additional_tokens += partial_tokens
                            break
                    
                    # åœ¨ç»“æŸæ ‡è®°å‰æ’å…¥é¢å¤–å†…å®¹
                    if additional_content:
                        end_pos = adjusted_content.rfind("[END]")
                        if end_pos != -1:
                            adjusted_content = adjusted_content[:end_pos] + "\n\n" + "\n\n".join(additional_content) + adjusted_content[end_pos:]
                            final_tokens = self._count_tokens(adjusted_content)
                            if self.show_detail:
                                print(f"   [DEBUG] æ·»åŠ é¢å¤–å†…å®¹å: {final_tokens:,} tokens")
                else:
                    # éœ€è¦åˆ é™¤å†…å®¹ - ä½¿ç”¨æ›´ç²¾ç¡®çš„äºŒåˆ†æŸ¥æ‰¾
                    adjusted_content = self._fine_tune_content_length(adjusted_content, target_tokens)
                    final_tokens = self._count_tokens(adjusted_content)
                    if self.show_detail:
                        print(f"   [DEBUG] ç²¾ç»†è°ƒæ•´å: {final_tokens:,} tokens")
            
            return adjusted_content, random_numbers
            
        except Exception as e:
            raise ContentGenerationError(f"ç”Ÿæˆæµ‹è¯•å†…å®¹å¤±è´¥: {str(e)}")
    
    def _replace_random_number_in_content(self, content: str, old_randoms: List[int], new_randoms: List[int]) -> str:
        """æ›¿æ¢æµ‹è¯•å†…å®¹ä¸­çš„éšæœºæ•°ï¼Œä¿æŒå…¶ä»–å†…å®¹ä¸å˜"""
        for i in range(len(old_randoms)):
            old_pattern = f"[random num: {old_randoms[i]}]"
            new_pattern = f"[random num: {new_randoms[i]}]"
            content = content.replace(old_pattern, new_pattern)
        
        return content
      
    def run_test(self) -> Optional[Dict[str, Any]]:
        """è¿è¡Œå®Œæ•´çš„ä¸Šä¸‹æ–‡é•¿åº¦æµ‹è¯•"""
        # æ£€æŸ¥ API Key
        if not self.api_key:
            print("âŒ é”™è¯¯ï¼šè¯·å…ˆè®¾ç½® API_KEY")
            print("æç¤ºï¼šåˆ›å»ºæµ‹è¯•å™¨æ—¶ä¼ å…¥ api_key å‚æ•°")
            return None
            
        print("ğŸš€ å¼€å§‹ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æµ‹è¯•")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"API åœ°å€: {self.api_url}")
        print(f"æ¨¡å‹: {self.model}")
        print(f"æµ‹è¯•è½®æ•°: {self.test_rounds}")
        print(f"å•è¯·æ±‚è¶…æ—¶: {self.timeout}ç§’")
        print(f"æµ‹è¯•å¤§å°: {self.test_tokens_list}")
        
        results = {}
        max_successful_tokens = 0
        
        for target_tokens in self.test_tokens_list:
            # ç”Ÿæˆæµ‹è¯•å†…å®¹
            test_content, random_numbers = self._generate_test_content(target_tokens)
            actual_tokens = self._count_tokens(test_content)
            
            # ä¿å­˜æµ‹è¯•å†…å®¹åˆ°logç›®å½•
            # è·å–ç”¨æˆ·è¾“å…¥çš„å¤§å°åç§°
            size_name = self._get_size_name(target_tokens)
            self._save_test_content(size_name, test_content, random_numbers)
            
            print(f"\nğŸ”„ æµ‹è¯•ä¸Šä¸‹æ–‡å¤§å°: {actual_tokens:,} tokens (ç›®æ ‡: {target_tokens:,})")
            print("=" * 60)
            
            # æ˜¾ç¤ºæ‰€æœ‰åˆ†è¯å™¨çš„ç»Ÿè®¡ç»“æœ
            all_tokenizer_results = self._count_tokens_all_tokenizers(test_content)
            print(f"   ç”Ÿæˆå†…å®¹é•¿åº¦ç»Ÿè®¡:")
            
            # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„åˆ†è¯å™¨ç»“æœï¼ˆåŠ ç²—æ˜¾ç¤ºï¼‰
            current_tokenizer_name = "GLM" if "glm" in self.model.lower() else "DeepSeek" if "deepseek" in self.model.lower() else "GLM"
            for tokenizer_name, token_count in all_tokenizer_results.items():
                if token_count is not None:
                    if tokenizer_name == current_tokenizer_name and self.use_tokenizer:
                        print(f"   â”œâ”€ {tokenizer_name}åˆ†è¯å™¨: {token_count:,} tokens âœ… (å½“å‰ä½¿ç”¨)")
                    else:
                        print(f"   â”œâ”€ {tokenizer_name}åˆ†è¯å™¨: {token_count:,} tokens")
                else:
                    print(f"   â”œâ”€ {tokenizer_name}åˆ†è¯å™¨: ä¸å¯ç”¨")
            
            print(f"   â”œâ”€ ç›®æ ‡tokenæ•°: {target_tokens:,}")
            print(f"   â””â”€ è¯¯å·®: {abs(actual_tokens - target_tokens):,} tokens ({abs(actual_tokens - target_tokens)/target_tokens*100:.1f}%)")
            print(f"   éšæœºæ•°: {random_numbers} (å…±{self.query_num}ä¸ª)")
            
            # æ˜¾ç¤ºåˆ†è¯å™¨ä¿¡æ¯
            if self.use_tokenizer:
                tokenizer_file = os.path.basename(self._get_tokenizer_file()) if self._get_tokenizer_file() else "æœªçŸ¥"
                tokenizer_info = f"æœ¬åœ°åˆ†è¯å™¨({tokenizer_file})"
            else:
                tokenizer_info = f"å­—ç¬¦ä¼°ç®—({self.chars_per_token}å­—ç¬¦/token)"
            print(f"   åˆ†è¯å™¨: {tokenizer_info}")
            
            # æ˜¾ç¤ºæµ‹è¯•å†…å®¹çš„é¦–å°¾éƒ¨åˆ†ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if target_tokens >= 32000:  # åªåœ¨è¾ƒå¤§çš„æµ‹è¯•æ—¶æ˜¾ç¤º
                start_preview = test_content[:200]
                end_preview = test_content[-200:]
                print(f"   å†…å®¹é¢„è§ˆ (å¼€å§‹): {repr(start_preview)}")
                print(f"   å†…å®¹é¢„è§ˆ (ç»“æŸ): {repr(end_preview)}")
                # import ipdb; ipdb.set_trace()  # è°ƒè¯•æ–­ç‚¹
            
            # è¿è¡Œæµ‹è¯•
            result = self._test_single_size(target_tokens, test_content, random_numbers, actual_tokens)
            results[target_tokens] = result
            
            # è®°å½•æˆåŠŸçš„æœ€å¤§ tokens
            if result.success:
                max_successful_tokens = target_tokens
                print(f"   âœ… æµ‹è¯•æˆåŠŸï¼")
            else:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {result.errors[0] if result.errors else 'æœªçŸ¥é”™è¯¯'}")
                print(f"\nâš ï¸  åœ¨ {target_tokens:,} tokens å¤„æµ‹è¯•å¤±è´¥")
                break
            
            # æˆåŠŸåçŸ­æš‚ä¼‘æ¯
            time.sleep(2)
        
        # æ‰“å°æ±‡æ€»æŠ¥å‘Š
        self._print_summary(results, max_successful_tokens)
        
        # å¯¼å‡ºç»“æœåˆ° JSON æ–‡ä»¶
        if self.output_file:
            self._export_results(results, max_successful_tokens)
        
        return {
            "results": results,
            "max_successful_tokens": max_successful_tokens
        }
    
    def _test_single_size(self, target_tokens: int, test_content: str, random_numbers: List[int], actual_tokens: int) -> ContextTestResult:
        """æµ‹è¯•å•ä¸ªä¸Šä¸‹æ–‡å¤§å°"""
        success_count = 0
        failure_count = 0
        errors = []
        response_times = []
        input_tokens_list = []
        
        for round_num in range(self.test_rounds):
            # ä¸ºæ¯è½®æµ‹è¯•ç”Ÿæˆæ–°çš„éšæœºæ•°ï¼Œä½†ä¿æŒå†…å®¹ç»“æ„ä¸å˜
            round_random_numbers = [random.randint(100, 999) for _ in range(self.query_num)]
            
            # æ›¿æ¢æµ‹è¯•å†…å®¹ä¸­çš„éšæœºæ•°
            round_test_content = self._replace_random_number_in_content(test_content, random_numbers, round_random_numbers)
            
            # æ˜¾ç¤ºæ­£ç¡®ç­”æ¡ˆï¼ˆä½¿ç”¨æ–°çš„éšæœºæ•°ï¼‰
            correct_answers = round_random_numbers
            print(f"   ç¬¬ {round_num + 1}/{self.test_rounds} è½®æµ‹è¯•... éšæœºæ•°: {correct_answers}")
            
            # ä¿å­˜æ¯è½®çš„æµ‹è¯•å†…å®¹
            if self.test_rounds > 1:  # åªåœ¨å¤šè½®æµ‹è¯•æ—¶ä¿å­˜
                # è·å–ç”¨æˆ·è¾“å…¥çš„å¤§å°åç§°
                size_name = self._get_size_name(target_tokens)
                self._save_round_content(size_name, round_num, round_test_content, round_random_numbers)
            
            success, response_time, error, model_answer, detail_data, input_tokens = self._make_single_request(round_test_content)
            
            # è®°å½•input_tokens
            if input_tokens:
                input_tokens_list.append(input_tokens)
            
            # æ˜¾ç¤ºæ¨¡å‹å›ç­”
            if model_answer:
                # é™åˆ¶æ¨¡å‹å›ç­”çš„æ˜¾ç¤ºé•¿åº¦
                if len(model_answer) > 200:
                    print(f"      æ¨¡å‹å›ç­”: {model_answer[:200]}...")
                else:
                    print(f"      æ¨¡å‹å›ç­”: {model_answer}")
                
                # éªŒè¯å›ç­”æ˜¯å¦æ­£ç¡®ï¼ˆæ£€æŸ¥éšæœºæ•°ï¼‰
                is_correct = False
                try:
                    # ä»å›ç­”ä¸­æå–æ‰€æœ‰æ•°å­—
                    import re
                    numbers_in_response = re.findall(r'random num:\s*(\d+)', model_answer.lower())
                    if not numbers_in_response:
                        # å°è¯•å…¶ä»–æ ¼å¼ - æŸ¥æ‰¾æ‰€æœ‰3ä½æ•°å­—
                        numbers_in_response = re.findall(r'\b\d{3}\b', model_answer)
                    
                    # è½¬æ¢ä¸ºæ•´æ•°
                    model_numbers = [int(n) for n in numbers_in_response]
                    
                    # ä¸å»é‡ï¼Œä¿ç•™æ‰€æœ‰æ‰¾åˆ°çš„æ•°å­—ï¼ˆåŒ…æ‹¬é‡å¤çš„ï¼‰
                    
                    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†æ‰€æœ‰éšæœºæ•°ï¼ˆè€ƒè™‘é‡å¤ï¼‰
                    # åˆ›å»ºæ­£ç¡®ç­”æ¡ˆçš„å‰¯æœ¬ç”¨äºåŒ¹é…
                    remaining_answers = correct_answers.copy()
                    found_count = 0
                    found_numbers = []
                    
                    # æŒ‰é¡ºåºåŒ¹é…æ¨¡å‹å›ç­”ä¸­çš„æ•°å­—
                    for num in model_numbers:
                        if num in remaining_answers:
                            found_count += 1
                            found_numbers.append(num)
                            remaining_answers.remove(num)
                    
                    missing_numbers = remaining_answers
                    extra_numbers = [n for n in model_numbers if n not in correct_answers]
                    
                    # è®¡ç®—å®é™…éœ€è¦çš„åŒ¹é…æ•°ï¼ˆè€ƒè™‘é‡å¤ï¼‰
                    required_count = len(correct_answers)
                    
                    if found_count >= required_count:
                        is_correct = True
                        print(f"      å›ç­”âœ…æ­£ç¡® (æ‰¾åˆ°{found_count}/{required_count}ä¸ªéšæœºæ•°)")
                        if extra_numbers:
                            print(f"      é¢å¤–æ‰¾åˆ°çš„æ•°å­—: {extra_numbers}")
                    else:
                        is_correct = False
                        print(f"      å›ç­”âŒé”™è¯¯ (æ‰¾åˆ°{found_count}/{required_count}ä¸ªéšæœºæ•°)")
                        if self.show_detail:  # åªåœ¨detailæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                            if found_numbers:
                                print(f"      æ­£ç¡®æ‰¾åˆ°çš„æ•°å­—: {found_numbers}")
                            if missing_numbers:
                                print(f"      é—æ¼çš„æ•°å­—: {missing_numbers}")
                            if extra_numbers:
                                print(f"      é¢å¤–çš„æ•°å­—: {extra_numbers}")
                            print(f"      æ­£ç¡®çš„æ•°å­—: {correct_answers}")
                            print(f"      æ¨¡å‹æ‰¾åˆ°çš„æ•°å­—: {model_numbers}")
                except Exception as e:
                    is_correct = False
                    print(f"      å›ç­”âŒé”™è¯¯ (è§£æå¤±è´¥: {str(e)})")
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.show_detail and detail_data:
                    print(f"      === è¯¦ç»†ä¿¡æ¯ ===")
                    print(f"      å“åº”å†…å®¹: {json.dumps(detail_data, ensure_ascii=False, indent=2)}")
                    print(f"      ===============")
                
                # æ›´æ–°æˆåŠŸçŠ¶æ€ï¼šéœ€è¦APIè¯·æ±‚æˆåŠŸä¸”å›ç­”æ­£ç¡®
                if success and is_correct:
                    success_count += 1
                else:
                    failure_count += 1
                    if not is_correct:
                        errors.append(f"æ¨¡å‹å›ç­”é”™è¯¯ï¼šåªæ‰¾åˆ°{found_count}/{self.query_num}ä¸ªéšæœºæ•°")
                    else:
                        errors.append(error if error else "è¯·æ±‚å¤±è´¥")
            else:
                print(f"      æ¨¡å‹å›ç­”: [ç©º] âŒé”™è¯¯")
                if success:
                    failure_count += 1
                    errors.append("æ¨¡å‹å›ç­”ä¸ºç©º")
                else:
                    failure_count += 1
                    errors.append(error if error else "è¯·æ±‚å¤±è´¥")
            
            response_times.append(response_time)
        
        # è®¡ç®—å‹ç¼©æ£€æµ‹
        avg_input_tokens = None
        is_compressed = False
        compression_ratio = None
        
        if input_tokens_list and actual_tokens:
            avg_input_tokens = int(sum(input_tokens_list) / len(input_tokens_list))
            # æ£€æŸ¥æ˜¯å¦å‹ç¼©ï¼ˆè¯¯å·®è¶…è¿‡15%ï¼‰
            compression_diff = abs(avg_input_tokens - actual_tokens)
            compression_ratio = compression_diff / actual_tokens * 100
            is_compressed = compression_ratio > 15
            
            if self.show_detail or is_compressed:
                print(f"   å‹ç¼©æ£€æµ‹: å‘é€tokens={actual_tokens:,}, æ¥æ”¶tokens={avg_input_tokens:,}, å·®å¼‚={compression_diff:,} ({compression_ratio:.1f}%)")
                if is_compressed:
                    print(f"   âš ï¸  æ£€æµ‹åˆ°å‹ç¼©ï¼å·®å¼‚è¶…è¿‡15%é˜ˆå€¼")
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = success_count / self.test_rounds if self.test_rounds > 0 else 0
        
        # åªæœ‰å½“æˆåŠŸç‡è¶…è¿‡50%æ—¶æ‰è®¤ä¸ºæµ‹è¯•æˆåŠŸ
        test_success = success_rate >= 0.5
        
        return ContextTestResult(
            success=test_success,
            success_count=success_count,
            failure_count=failure_count,
            avg_response_time=statistics.mean(response_times),
            errors=errors,
            actual_tokens=actual_tokens,
            input_tokens=avg_input_tokens,
            is_compressed=is_compressed,
            compression_ratio=compression_ratio
        )
    
    def _make_single_request(self, test_content: str) -> Tuple[bool, float, Optional[str], Optional[str], Optional[Dict[str, Any]], Optional[int]]:
        """å‘é€å•ä¸ªæµ‹è¯•è¯·æ±‚"""
        start_time = time.time()
        
        # è®¾ç½®è¯·æ±‚å¤´
        if self.use_gemini_api:
            headers = {
                "x-goog-api-key": self.api_key,
                "Content-Type": "application/json",
            }
            payload = {
                "contents": [{
                    "role": "user",
                    "parts": [{
                        "text": "You are a context testing assistant. Your only task is to read the entire text and report all random numbers hidden in it. Respond with the numbers you found, using the format: random num: xxx\n\n" + test_content
                    }]
                }],
                "generationConfig": {
                    "maxOutputTokens": 2048,
                    "temperature": 0.1,
                }
            }
            print(f"      ä½¿ç”¨ Gemini API æ ¼å¼å‘é€è¯·æ±‚")
        elif self.use_chat_api:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            payload = {
                "model": self.model,
                "max_tokens": 2048,
                "temperature": 0.1,
                "messages": [
                    {"role": "system", "content": "You are a context testing assistant. Your only task is to read the entire text and report all random numbers hidden in it. Respond with the numbers you found, using the format: random num: xxx"},
                    {"role": "user", "content": test_content}
                ]
            }
            
            # å¯¹äº GLM æ¨¡å‹ï¼Œå…³é—­æ€è€ƒæ¨¡å¼ä»¥è·å¾—æ›´å¿«å“åº”
            should_disable_thinking = (
                self.disable_thinking is True or 
                (self.disable_thinking is None and "glm" in self.model.lower())
            )
            
            if should_disable_thinking:
                payload["thinking"] = {"type": "disabled"}
                print(f"      å·²è®¾ç½® thinking.type = disabledï¼ˆGLM æ€è€ƒæ¨¡å¼å…³é—­ï¼‰")
                
        else:
            headers = {
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            }
            payload = {
                "model": self.model,
                "max_tokens": 2048,
                "temperature": 0.1,
                "messages": [
                    {"role": "user", "content": test_content}
                ]
            }
        
        model_answer = None
        input_tokens = None
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                data=json.dumps(payload),
                timeout=self.timeout
            )
            
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                # æ£€æŸ¥å“åº”å†…å®¹
                try:
                    response_data = response.json()
                except json.JSONDecodeError:
                    return (False, response_time, "å“åº”JSONè§£æå¤±è´¥", model_answer, None, None)
                
                # æå–input_tokens
                if self.use_gemini_api:
                    # Gemini APIæ ¼å¼
                    content = ""
                    
                    # è°ƒè¯•ï¼šè¾“å‡ºåŸå§‹å“åº”æ•°æ®ï¼ˆä»…åœ¨detailæ¨¡å¼ä¸‹ï¼‰
                    if self.show_detail:
                        print(f"      [DEBUG] Gemini åŸå§‹å“åº”ç±»å‹: {type(response_data)}")
                        if isinstance(response_data, list) and len(response_data) > 0:
                            print(f"      [DEBUG] ç¬¬ä¸€é¡¹æ•°æ®: {json.dumps(response_data[0], ensure_ascii=False, indent=2)[:500]}...")
                    
                    # å¤„ç†æµå¼å“åº”æ ¼å¼ï¼ˆGemini è¿”å›çš„æ˜¯æ•°ç»„ï¼‰
                    if isinstance(response_data, list):
                        # åˆå¹¶æ‰€æœ‰æµå¼å“åº”çš„å†…å®¹
                        for item in response_data:
                            if isinstance(item, dict) and "candidates" in item:
                                candidates = item.get("candidates", [])
                                if candidates and len(candidates) > 0:
                                    parts = candidates[0].get("content", {}).get("parts", [])
                                    if parts and len(parts) > 0:
                                        content += parts[0].get("text", "")
                        
                        # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå“åº”çš„ usageMetadata
                        for item in response_data:
                            if isinstance(item, dict) and "usageMetadata" in item:
                                usage_metadata = item.get("usageMetadata", {})
                                if isinstance(usage_metadata, dict):
                                    input_tokens = usage_metadata.get("promptTokenCount")
                                    break
                        else:
                            input_tokens = None
                    else:
                        # æ ‡å‡†å“åº”æ ¼å¼ï¼ˆå¤‡ç”¨ï¼‰
                        candidates = response_data.get("candidates", [])
                        if candidates and len(candidates) > 0:
                            parts = candidates[0].get("content", {}).get("parts", [])
                            if parts and len(parts) > 0:
                                content = parts[0].get("text", "")
                        
                        usage_metadata = response_data.get("usageMetadata", {})
                        if isinstance(usage_metadata, dict):
                            input_tokens = usage_metadata.get("promptTokenCount")
                        else:
                            input_tokens = None
                    
                elif self.use_chat_api:
                    # Chat APIæ ¼å¼
                    usage = response_data.get("usage", {})
                    input_tokens = usage.get("prompt_tokens")
                    content = response_data.get("choices", [{}])[0].get("message", {}).get("content", "")
                else:
                    # Anthropic APIæ ¼å¼
                    usage = response_data.get("usage", {})
                    input_tokens = usage.get("input_tokens")
                    content = ""
                    content_blocks = response_data.get("content", [])
                    for block in content_blocks:
                        if block.get("type") == "text":
                            content += block.get("text", "")
                
                # æå–å®Œæ•´æ¨¡å‹å›ç­”
                model_answer = content
                
                # éªŒè¯å“åº”æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹
                if content and len(content.strip()) > 0:
                    # æœ‰å“åº”å†…å®¹ï¼Œä½†å…·ä½“æ˜¯å¦æ­£ç¡®éœ€è¦åœ¨ä¸Šçº§æ–¹æ³•ä¸­éªŒè¯
                    detail_data = response_data if self.show_detail else None
                    return (True, response_time, None, model_answer, detail_data, input_tokens)
                else:
                    return (False, response_time, "å“åº”å†…å®¹ä¸ºç©º", model_answer, None, input_tokens)
            elif response.status_code == 429:
                return (False, response_time, "API è¯·æ±‚é¢‘ç‡é™åˆ¶", model_answer, None, None)
            elif response.status_code == 401:
                return (False, response_time, "API å¯†é’¥æ— æ•ˆ", model_answer, None, None)
            elif response.status_code == 400:
                error_text = response.text[:200] if response.text else ""
                return (False, response_time, f"è¯·æ±‚å‚æ•°é”™è¯¯: {error_text}", model_answer, None, None)
            elif response.status_code >= 500:
                return (False, response_time, f"æœåŠ¡å™¨é”™è¯¯: HTTP {response.status_code}", model_answer, None, None)
            else:
                error_text = response.text[:200] if response.text else ""
                return (False, response_time, f"HTTP {response.status_code}: {error_text}", model_answer, None, None)
                
        except requests.Timeout:
            response_time = time.time() - start_time
            return (False, response_time, f"è¯·æ±‚è¶…æ—¶ï¼ˆ{self.timeout}ç§’ï¼‰", model_answer, None, None)
        except requests.ConnectionError:
            response_time = time.time() - start_time
            return (False, response_time, "ç½‘ç»œè¿æ¥é”™è¯¯", model_answer, None, None)
        except requests.RequestException as e:
            response_time = time.time() - start_time
            return (False, response_time, f"è¯·æ±‚å¼‚å¸¸: {str(e)}", model_answer, None, None)
        except Exception as e:
            response_time = time.time() - start_time
            return (False, response_time, f"æœªçŸ¥é”™è¯¯: {str(e)}", model_answer, None, None)
    
    def _print_summary(self, results: Dict[int, ContextTestResult], max_successful_tokens: int) -> None:
        """æ‰“å°æµ‹è¯•æ±‡æ€»æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print(f"ğŸ“‹ ä¸Šä¸‹æ–‡çª—å£æµ‹è¯•æ±‡æ€»")
        print("=" * 80)
        print("\nä¸Šä¸‹æ–‡å¤§å° | æˆåŠŸç‡ | å¹³å‡å“åº”æ—¶é—´ | å‹ç¼©çŠ¶æ€ | æˆåŠŸ/å¤±è´¥æ¬¡æ•°")
        print("-" * 85)
        
        for tokens, result in results.items():
            # è®¡ç®—æˆåŠŸç‡
            total_requests = result.success_count + result.failure_count
            success_rate = (result.success_count / total_requests * 100) if total_requests > 0 else 0.0
            
            # å¹³å‡å“åº”æ—¶é—´
            avg_time = f"{result.avg_response_time:.2f}s" if result.avg_response_time else "N/A"
            
            # å‹ç¼©çŠ¶æ€
            if result.is_compressed:
                compression_status = f"âš ï¸ å‹ç¼©({result.compression_ratio:.1f}%)"
            elif result.input_tokens:
                compression_status = f"âœ… æ­£å¸¸({abs(result.input_tokens - result.actual_tokens)/result.actual_tokens*100:.1f}%)"
            else:
                compression_status = "æœªçŸ¥"
            
            # æˆåŠŸ/å¤±è´¥æ¬¡æ•°
            attempts = f"{result.success_count}/{result.failure_count}"
            
            # ä½¿ç”¨å®é™…tokensè€Œä¸æ˜¯ç›®æ ‡tokens
            display_tokens = result.actual_tokens if result.actual_tokens is not None else tokens
            print(f"{display_tokens:9,} | {success_rate:6.1f}% | {avg_time:13s} | {compression_status:15s} | {attempts}")
        
        print(f"\nğŸ¯ æµ‹è¯•ç»“æœ:")
        print(f"   æœ€å¤§æˆåŠŸä¸Šä¸‹æ–‡: {max_successful_tokens:,} tokens")
        
        # é¢å¤–ç»Ÿè®¡ä¿¡æ¯
        total_tests = sum(result.success_count + result.failure_count for result in results.values())
        total_success = sum(result.success_count for result in results.values())
        total_failure = sum(result.failure_count for result in results.values())
        overall_success_rate = (total_success / total_tests * 100) if total_tests > 0 else 0.0
        
        print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ¬¡æ•°: {total_tests}")
        print(f"   æ€»æˆåŠŸæ¬¡æ•°: {total_success}")
        print(f"   æ€»å¤±è´¥æ¬¡æ•°: {total_failure}")
        print(f"   æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%")
        
        # å‹ç¼©ç»Ÿè®¡
        compressed_tests = [result for result in results.values() if result.is_compressed]
        if compressed_tests:
            print(f"\nğŸ—œï¸  å‹ç¼©æ£€æµ‹:")
            print(f"   æ£€æµ‹åˆ°å‹ç¼©çš„æµ‹è¯•: {len(compressed_tests)}")
            print(f"   å¹³å‡å‹ç¼©ç‡: {sum(result.compression_ratio for result in compressed_tests) / len(compressed_tests):.1f}%")
            print(f"   æœ€å¤§å‹ç¼©ç‡: {max(result.compression_ratio for result in compressed_tests):.1f}%")
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´ï¼ˆä»…æˆåŠŸæµ‹è¯•ï¼‰
        successful_times = [result.avg_response_time for result in results.values() if result.success and result.avg_response_time]
        if successful_times:
            avg_successful_time = sum(successful_times) / len(successful_times)
            print(f"   å¹³å‡å“åº”æ—¶é—´ï¼ˆæˆåŠŸï¼‰: {avg_successful_time:.2f}s")
        
        # å¦‚æœæœ‰å¤±è´¥çš„æµ‹è¯•ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
        failed_results = [(tokens, result) for tokens, result in results.items() if not result.success]
        if failed_results:
            print(f"\nâŒ å¤±è´¥è¯¦æƒ…:")
            for tokens, result in failed_results:
                if result.errors:
                    print(f"   {tokens:,} tokens: {result.errors[0]}")

    def _export_results(self, results: Dict[int, ContextTestResult], max_successful_tokens: int) -> None:
        """å¯¼å‡ºæµ‹è¯•ç»“æœåˆ° JSON æ–‡ä»¶"""
        export_data = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "api_url": self.api_url,
                "model": self.model,
                "test_rounds": self.test_rounds,
                "timeout": self.timeout,
                "chars_per_token": self.chars_per_token
            },
            "results": {},
            "summary": {
                "max_successful_tokens": max_successful_tokens,
                "max_successful_chars": int(max_successful_tokens * self.chars_per_token),
                "max_successful_chinese_chars": int(max_successful_tokens * self.chars_per_token / 2),
                "max_successful_english_words": int(max_successful_tokens * self.chars_per_token / 5)
            }
        }
        
        # è½¬æ¢ç»“æœæ•°æ®
        for tokens, result in results.items():
            export_data["results"][str(tokens)] = {
                "success": result.success,
                "success_count": result.success_count,
                "failure_count": result.failure_count,
                "avg_response_time": result.avg_response_time,
                "errors": result.errors,
                "actual_tokens": result.actual_tokens,
                "input_tokens": result.input_tokens,
                "is_compressed": result.is_compressed,
                "compression_ratio": result.compression_ratio
            }
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ æµ‹è¯•ç»“æœå·²å¯¼å‡ºåˆ°: {self.output_file}")
        except Exception as e:
            print(f"\nâŒ å¯¼å‡ºç»“æœå¤±è´¥: {str(e)}")
    
    def _validate_response_content(self, content: str, random_number: int) -> bool:
        """éªŒè¯å“åº”å†…å®¹æ˜¯å¦åŒ…å«æ­£ç¡®çš„éšæœºæ•°"""
        import re
        
        # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
        numbers = re.findall(r'\b(\d{3})\b', content)  # æŸ¥æ‰¾3ä½æ•°å­—
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡éšæœºæ•°
        return str(random_number) in numbers
    
    def _get_base_text(self) -> str:
        """è·å–åŸºç¡€æµ‹è¯•æ–‡æœ¬"""
        return """è¿™æ˜¯ä¸€æ®µç”¨äºæµ‹è¯•ä¸Šä¸‹æ–‡çª—å£é•¿åº¦çš„ç¤ºä¾‹æ–‡æœ¬ã€‚æ¯ä¸ªæ®µè½éƒ½åŒ…å«ç›¸åŒçš„å†…å®¹ï¼Œä»¥ä¾¿äºéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¤„ç†å®Œæ•´çš„ä¸Šä¸‹æ–‡ã€‚
        
ä¸Šä¸‹æ–‡çª—å£æ˜¯æŒ‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€æ¬¡äº¤äº’ä¸­èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§æ–‡æœ¬é‡ã€‚å®ƒåŒ…æ‹¬äº†è¾“å…¥æç¤ºã€ç³»ç»Ÿæç¤ºä»¥åŠæ¨¡å‹ç”Ÿæˆçš„å“åº”ã€‚æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£å…è®¸æ¨¡å‹è€ƒè™‘æ›´å¤šçš„ä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®å’Œè¿è´¯çš„å›ç­”ã€‚

æµ‹è¯•ä¸Šä¸‹æ–‡çª—å£çš„é‡è¦æ€§åœ¨äºï¼š
1. ç¡®å®šæ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§æ–‡æœ¬é‡
2. è¯„ä¼°æ¨¡å‹åœ¨é•¿æ–‡æœ¬ä¸‹çš„è¡¨ç°
3. éªŒè¯æ¨¡å‹çš„å®é™…èƒ½åŠ›ä¸å®£ä¼ æ˜¯å¦ä¸€è‡´
4. ä¸ºåº”ç”¨å¼€å‘æä¾›å‚è€ƒä¾æ®

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¸Šä¸‹æ–‡çª—å£çš„å¤§å°ç›´æ¥å½±å“ï¼š
- æ–‡æ¡£æ€»ç»“çš„èƒ½åŠ›
- å¤šè½®å¯¹è¯çš„è¿è´¯æ€§
- ä»£ç åˆ†æçš„èŒƒå›´
- æ•°æ®å¤„ç†çš„æ•ˆç‡

"""
    
    def _get_alternative_base_text(self, paragraph_num: int) -> str:
        """ç”Ÿæˆéšæœºæµ‹è¯•æ–‡æœ¬ï¼Œé¿å…é‡å¤å†…å®¹è§¦å‘è¿‡æ»¤"""
        # å¦‚æœè¶…è¿‡æœ€å¤§æ®µè½æ•°é™åˆ¶ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if paragraph_num >= self.max_paragraphs:
            return ""
            
        # ä½¿ç”¨å…¨å±€éšæœºæ•°ç”Ÿæˆå™¨ï¼Œç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½ä¸åŒ
        # ä¸é‡æ–°è®¾ç½®ç§å­ï¼Œè®©randomæ¨¡å—ä½¿ç”¨ç³»ç»Ÿæ—¶é—´
        
        if self.use_english:
            # æ‰©å±•è‹±æ–‡è¯æ±‡åº“
            subjects = [
                "Artificial intelligence", "Machine learning", "Deep learning", "Neural networks", 
                "Data science", "Cloud computing", "Blockchain", "Quantum computing",
                "Cybersecurity", "Internet of Things", "Computer vision", "Robotics",
                "Natural language processing", "Edge computing", "Big data analytics",
                "Augmented reality", "Virtual reality", "5G technology", "Autonomous vehicles",
                "Smart cities", "Digital transformation", "Fintech", "Biotechnology",
                "Renewable energy", "Nanotechnology", "3D printing", "Drones",
                "Wearable technology", "Voice assistants", "Predictive analytics", "DevOps"
            ]
            
            verbs = [
                "transforms", "enhances", "revolutionizes", "optimizes", "improves", "automates",
                "streamlines", "innovates", "accelerates", "modernizes", "digitizes", "integrates",
                "facilitates", "enables", "empowers", "simplifies", "standardizes", "customizes",
                "democratizes", "disrupts", "catalyzes", "amplifies", "orchestrates", "synchronizes"
            ]
            
            objects = [
                "business processes", "healthcare systems", "financial services", "manufacturing",
                "education platforms", "retail operations", "transportation networks", "energy grids",
                "communication systems", "supply chains", "customer experiences", "workflows",
                "decision making", "risk management", "quality control", "compliance monitoring",
                "resource allocation", "talent acquisition", "knowledge management", "innovation pipelines",
                "market research", "product development", "service delivery", "stakeholder engagement"
            ]
            
            adjectives = [
                "advanced", "sophisticated", "cutting-edge", "innovative", "revolutionary",
                "state-of-the-art", "modern", "efficient", "scalable", "robust", "flexible", "intelligent",
                "autonomous", "adaptive", "responsive", "proactive", "dynamic", "seamless",
                "comprehensive", "holistic", "integrated", "unified", "centralized", "distributed"
            ]
            
            # æŠ€æœ¯ç‰¹å®šè¯æ±‡
            tech_terms = [
                "algorithm", "framework", "architecture", "protocol", "ecosystem", "paradigm",
                "methodology", "infrastructure", "platform", "solution", "approach", "strategy",
                "implementation", "deployment", "integration", "migration", "optimization", "scalability"
            ]
            
            # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
            paragraph_id = f"P{paragraph_num:04d}"
            
            # ç”Ÿæˆéšæœºæ•°é‡çš„å¥å­
            num_sentences = random.randint(4, 8)
            sentences = []
            used_combinations = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„ç»„åˆ
            
            for i in range(num_sentences):
                # å°è¯•æ‰¾åˆ°æœªä½¿ç”¨çš„ç»„åˆ
                max_attempts = 10
                for attempt in range(max_attempts):
                    subject = random.choice(subjects)
                    verb = random.choice(verbs)
                    obj = random.choice(objects)
                    adj = random.choice(adjectives)
                    
                    # åˆ›å»ºç»„åˆé”®
                    combo_key = f"{subject[:8]}_{verb[:5]}_{obj[:8]}_{adj[:5]}"
                    
                    if combo_key not in used_combinations or attempt == max_attempts - 1:
                        used_combinations.add(combo_key)
                        break
                
                if i == 0:
                    sentence = f"{paragraph_id}: {subject} {verb} {obj} through {adj} technologies and methodologies."
                else:
                    # æ›´å¤šçš„å¥å­ç±»å‹å˜åŒ–
                    sentence_templates = [
                        f"The implementation of {subject.lower()} requires careful consideration of {random.choice(['scalability', 'performance', 'security', 'reliability'])} and {random.choice(['compliance', 'governance', 'standards', 'best practices'])}.",
                        f"Recent advances in {subject.lower()} have opened new possibilities for {obj} optimization through {random.choice(tech_terms)} integration.",
                        f"Organizations adopting {subject.lower()} report significant improvements in {random.choice(['efficiency', 'productivity', 'ROI', 'TCO'])} and {random.choice(['agility', 'resilience', 'innovation', 'growth'])}.",
                        f"The future of {subject.lower()} depends on continued research and development in {random.choice(tech_terms)} and {random.choice(['emerging technologies', 'industry standards', 'regulatory frameworks', 'market demands'])}.",
                        f"Integration of {subject.lower()} with existing systems presents both {random.choice(['challenges', 'opportunities', 'risks', 'benefits'])} and {random.choice(['advantages', 'disadvantages', 'trade-offs', 'synergies'])}.",
                        f"Industry experts predict that {subject.lower()} will {random.choice(['disrupt', 'transform', 'redefine', 'reshape'])} the way we approach {obj} in the coming years.",
                        f"Case studies show that successful {subject.lower()} implementations can achieve up to {random.randint(20, 95)}% improvement in key performance indicators.",
                        f"The {random.choice(['ROI', 'TCO', 'NPV', 'IRR'])} of {subject.lower()} projects typically ranges from {random.randint(100, 999)}% depending on the scope and scale."
                    ]
                    sentence = random.choice(sentence_templates)
                
                sentences.append(sentence)
            
            # æ·»åŠ æ®µè½ç‰¹æœ‰çš„é¢å¤–å†…å®¹
            extra_content_types = [
                f"Industry analysts project the global market for these technologies will reach ${random.randint(1, 999)} billion by {2025 + random.randint(1, 10)}.",
                f"A recent survey of {random.randint(100, 999)} organizations revealed that {random.randint(20, 95)}% are planning to increase investment in this area.",
                f"The adoption rate has increased by {random.randint(20, 300)}% year-over-year, indicating strong market momentum.",
                f"Leading vendors in this space include both established technology giants and innovative startups funded with over ${random.randint(10, 500)} million in venture capital."
            ]
            
            if random.random() > 0.6:
                sentences.append(random.choice(extra_content_types))
            
            # ç»„åˆæˆæ®µè½
            paragraph = " ".join(sentences)
            
            return f"""{paragraph}

This unique content block {paragraph_id} demonstrates the model's ability to process entirely original textual information without repetition patterns. Each paragraph is algorithmically generated to ensure maximum uniqueness.

The sophisticated randomization algorithm combines vocabulary permutation with structural variation to create content that cannot be predicted or memorized by language models.

Content uniqueness verification: {hash(paragraph) % 1000000:06d}

"""
        else:
            # æ‰©å±•ä¸­æ–‡è¯æ±‡åº“
            subjects = [
                "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ•°æ®ç§‘å­¦", "äº‘è®¡ç®—",
                "åŒºå—é“¾", "é‡å­è®¡ç®—", "ç½‘ç»œå®‰å…¨", "ç‰©è”ç½‘", "è®¡ç®—æœºè§†è§‰", "æœºå™¨äººæŠ€æœ¯",
                "è‡ªç„¶è¯­è¨€å¤„ç†", "è¾¹ç¼˜è®¡ç®—", "å¤§æ•°æ®åˆ†æ", "å¢å¼ºç°å®", "è™šæ‹Ÿç°å®",
                "5GæŠ€æœ¯", "è‡ªåŠ¨é©¾é©¶", "æ™ºæ…§åŸå¸‚", "æ•°å­—åŒ–è½¬å‹", "é‡‘èç§‘æŠ€", "ç”Ÿç‰©æŠ€æœ¯",
                "å¯å†ç”Ÿèƒ½æº", "çº³ç±³æŠ€æœ¯", "3Dæ‰“å°", "æ— äººæœº", "å¯ç©¿æˆ´è®¾å¤‡", "è¯­éŸ³åŠ©æ‰‹",
                "é¢„æµ‹åˆ†æ", "DevOps", "å¾®æœåŠ¡", "å®¹å™¨åŒ–", "æ— æœåŠ¡å™¨è®¡ç®—", "æ•°å­—åŒ–è½¬å‹"
            ]
            
            verbs = [
                "æ”¹å˜äº†", "æå‡äº†", "é©æ–°äº†", "ä¼˜åŒ–äº†", "æ”¹è¿›äº†", "è‡ªåŠ¨åŒ–äº†",
                "ç®€åŒ–äº†", "åˆ›æ–°äº†", "åŠ é€Ÿäº†", "ç°ä»£åŒ–äº†", "æ•°å­—åŒ–äº†", "é›†æˆäº†",
                "ä¿ƒè¿›äº†", "å®ç°äº†", "èµ‹èƒ½äº†", "æ ‡å‡†åŒ–äº†", "å®šåˆ¶åŒ–äº†", "æ™®åŠäº†",
                "é¢ è¦†äº†", "å‚¬åŒ–äº†", "æ”¾å¤§äº†", "åè°ƒäº†", "åŒæ­¥äº†", "é‡æ„äº†"
            ]
            
            objects = [
                "ä¸šåŠ¡æµç¨‹", "åŒ»ç–—ç³»ç»Ÿ", "é‡‘èæœåŠ¡", "åˆ¶é€ ä¸š", "æ•™è‚²å¹³å°", "é›¶å”®è¿è¥",
                "äº¤é€šç½‘ç»œ", "èƒ½æºç½‘ç»œ", "é€šä¿¡ç³»ç»Ÿ", "ä¾›åº”é“¾", "å®¢æˆ·ä½“éªŒ", "å·¥ä½œæµç¨‹",
                "å†³ç­–åˆ¶å®š", "é£é™©ç®¡ç†", "è´¨é‡æ§åˆ¶", "åˆè§„ç›‘æ§", "èµ„æºåˆ†é…", "äººæ‰è·å–",
                "çŸ¥è¯†ç®¡ç†", "åˆ›æ–°ç®¡é“", "å¸‚åœºç ”ç©¶", "äº§å“å¼€å‘", "æœåŠ¡äº¤ä»˜", "åˆ©ç›Šç›¸å…³è€…å‚ä¸"
            ]
            
            adjectives = [
                "å…ˆè¿›çš„", "å¤æ‚çš„", "å°–ç«¯çš„", "åˆ›æ–°çš„", "é©å‘½æ€§çš„", "æœ€å…ˆè¿›çš„",
                "ç°ä»£çš„", "é«˜æ•ˆçš„", "å¯æ‰©å±•çš„", "ç¨³å¥çš„", "çµæ´»çš„", "æ™ºèƒ½çš„",
                "è‡ªä¸»çš„", "è‡ªé€‚åº”çš„", "å“åº”å¼çš„", "ä¸»åŠ¨çš„", "åŠ¨æ€çš„", "æ— ç¼çš„",
                "å…¨é¢çš„", "æ•´ä½“çš„", "é›†æˆçš„", "ç»Ÿä¸€çš„", "é›†ä¸­çš„", "åˆ†å¸ƒå¼çš„"
            ]
            
            # æŠ€æœ¯ç‰¹å®šè¯æ±‡
            tech_terms = [
                "ç®—æ³•", "æ¡†æ¶", "æ¶æ„", "åè®®", "ç”Ÿæ€ç³»ç»Ÿ", "èŒƒå¼",
                "æ–¹æ³•è®º", "åŸºç¡€è®¾æ–½", "å¹³å°", "è§£å†³æ–¹æ¡ˆ", "æ–¹æ³•", "ç­–ç•¥",
                "å®æ–½", "éƒ¨ç½²", "é›†æˆ", "è¿ç§»", "ä¼˜åŒ–", "å¯æ‰©å±•æ€§"
            ]
            
            # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºå”¯ä¸€æ ‡è¯†ç¬¦
            paragraph_id = f"ç¬¬{paragraph_num + 1:04d}æ®µ"
            
            # ç”Ÿæˆéšæœºæ•°é‡çš„å¥å­
            num_sentences = random.randint(4, 8)
            sentences = []
            used_combinations = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„ç»„åˆ
            
            for i in range(num_sentences):
                # å°è¯•æ‰¾åˆ°æœªä½¿ç”¨çš„ç»„åˆ
                max_attempts = 10
                for attempt in range(max_attempts):
                    subject = random.choice(subjects)
                    verb = random.choice(verbs)
                    obj = random.choice(objects)
                    adj = random.choice(adjectives)
                    
                    # åˆ›å»ºç»„åˆé”®
                    combo_key = f"{subject}_{verb}_{obj}_{adj}"
                    
                    if combo_key not in used_combinations or attempt == max_attempts - 1:
                        used_combinations.add(combo_key)
                        break
                
                if i == 0:
                    sentence = f"{paragraph_id}ï¼š{subject}{verb}{obj}ï¼Œé€šè¿‡{adj}æŠ€æœ¯å®ç°çªç ´æ€§è¿›å±•ã€‚"
                else:
                    # æ›´å¤šçš„å¥å­ç±»å‹å˜åŒ–
                    sentence_templates = [
                        f"{subject}çš„å®æ–½éœ€è¦ä»”ç»†è€ƒè™‘{random.choice(['å¯æ‰©å±•æ€§', 'æ€§èƒ½', 'å®‰å…¨æ€§', 'å¯é æ€§'])}å’Œ{random.choice(['åˆè§„æ€§', 'æ²»ç†', 'æ ‡å‡†', 'æœ€ä½³å®è·µ'])}ã€‚",
                        f"{subject}çš„æœ€æ–°è¿›å±•ä¸º{obj}çš„ä¼˜åŒ–å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨{random.choice(tech_terms)}é›†æˆæ–¹é¢ã€‚",
                        f"é‡‡ç”¨{subject}çš„ç»„ç»‡æŠ¥å‘Šç§°åœ¨{random.choice(['æ•ˆç‡', 'ç”Ÿäº§åŠ›', 'æŠ•èµ„å›æŠ¥ç‡', 'æ€»æ‹¥æœ‰æˆæœ¬'])}å’Œ{random.choice(['æ•æ·æ€§', 'éŸ§æ€§', 'åˆ›æ–°èƒ½åŠ›', 'å¢é•¿'])}æ–¹é¢æœ‰æ˜¾è‘—æ”¹å–„ã€‚",
                        f"{subject}çš„æœªæ¥å–å†³äºè¯¥é¢†åŸŸåœ¨{random.choice(tech_terms)}å’Œ{random.choice(['æ–°å…´æŠ€æœ¯', 'è¡Œä¸šæ ‡å‡†', 'ç›‘ç®¡æ¡†æ¶', 'å¸‚åœºéœ€æ±‚'])}æ–¹é¢çš„æŒç»­ç ”ç©¶å’Œå¼€å‘ã€‚",
                        f"å°†{subject}ä¸ç°æœ‰ç³»ç»Ÿé›†æˆæ—¢å¸¦æ¥äº†{random.choice(['æŒ‘æˆ˜', 'æœºé‡', 'é£é™©', 'æ”¶ç›Š'])}ï¼Œä¹Ÿåˆ›é€ äº†{random.choice(['ä¼˜åŠ¿', 'åŠ£åŠ¿', 'æƒè¡¡', 'ååŒæ•ˆåº”'])}ã€‚",
                        f"è¡Œä¸šä¸“å®¶é¢„æµ‹{subject}å°†åœ¨æœªæ¥å‡ å¹´å†…{random.choice(['é¢ è¦†', 'æ”¹å˜', 'é‡æ–°å®šä¹‰', 'é‡å¡‘'])}æˆ‘ä»¬å¤„ç†{obj}çš„æ–¹å¼ã€‚",
                        f"æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸçš„{subject}å®æ–½å¯ä»¥å®ç°å…³é”®ç»©æ•ˆæŒ‡æ ‡é«˜è¾¾{random.randint(20, 95)}%çš„æå‡ã€‚",
                        f"{subject}é¡¹ç›®çš„{random.choice(['æŠ•èµ„å›æŠ¥ç‡', 'æ€»æ‹¥æœ‰æˆæœ¬', 'å‡€ç°å€¼', 'å†…éƒ¨æ”¶ç›Šç‡'])}é€šå¸¸æ ¹æ®èŒƒå›´å’Œè§„æ¨¡åœ¨{random.randint(100, 999)}%ä¹‹é—´ã€‚"
                    ]
                    sentence = random.choice(sentence_templates)
                
                sentences.append(sentence)
            
            # æ·»åŠ æ®µè½ç‰¹æœ‰çš„é¢å¤–å†…å®¹
            extra_content_types = [
                f"è¡Œä¸šåˆ†æå¸ˆé¢„æµ‹ï¼Œè¿™äº›æŠ€æœ¯çš„å…¨çƒå¸‚åœºå°†åœ¨{2025 + random.randint(1, 10)}å¹´è¾¾åˆ°{random.randint(1, 999)}0äº¿ç¾å…ƒã€‚",
                f"æœ€è¿‘ä¸€é¡¹å¯¹{random.randint(100, 999)}å®¶ç»„ç»‡çš„è°ƒæŸ¥æ˜¾ç¤ºï¼Œ{random.randint(20, 95)}%è®¡åˆ’å¢åŠ è¯¥é¢†åŸŸçš„æŠ•èµ„ã€‚",
                f"é‡‡ç”¨ç‡åŒæ¯”å¢é•¿äº†{random.randint(20, 300)}%ï¼Œè¡¨æ˜å¸‚åœºåŠ¿å¤´å¼ºåŠ²ã€‚",
                f"è¯¥é¢†åŸŸçš„é¢†å…ˆä¾›åº”å•†æ—¢åŒ…æ‹¬æŠ€æœ¯å·¨å¤´ï¼Œä¹ŸåŒ…æ‹¬è·å¾—è¶…è¿‡{random.randint(10, 500)}äº¿ç¾å…ƒé£é™©æŠ•èµ„æ”¯æŒçš„åˆåˆ›ä¼ä¸šã€‚"
            ]
            
            if random.random() > 0.6:
                sentences.append(random.choice(extra_content_types))
            
            # ç»„åˆæˆæ®µè½
            paragraph = "".join(sentences)
            
            return f"""{paragraph}

è¿™æ®µå”¯ä¸€å†…å®¹å—{paragraph_id}å±•ç¤ºäº†æ¨¡å‹å¤„ç†å®Œå…¨åŸåˆ›æ–‡æœ¬ä¿¡æ¯è€Œæ— é‡å¤æ¨¡å¼çš„èƒ½åŠ›ã€‚æ¯ä¸ªæ®µè½éƒ½é€šè¿‡ç®—æ³•ç”Ÿæˆï¼Œç¡®ä¿æœ€å¤§ç¨‹åº¦çš„ç‹¬ç‰¹æ€§ã€‚

å¤æ‚çš„éšæœºåŒ–ç®—æ³•ç»“åˆè¯æ±‡æ’åˆ—å’Œç»“æ„å˜åŒ–ï¼Œåˆ›é€ å‡ºè¯­è¨€æ¨¡å‹æ— æ³•é¢„æµ‹æˆ–è®°å¿†çš„å†…å®¹ã€‚

å†…å®¹å”¯ä¸€æ€§éªŒè¯ï¼š{hash(paragraph) % 1000000:06d}

"""
    
    def _adjust_content_length(self, content: str, target_tokens: int) -> str:
        """è°ƒæ•´å†…å®¹é•¿åº¦ä»¥åŒ¹é…ç›®æ ‡ tokens"""
        current_tokens = self._count_tokens(content)
        
        if current_tokens <= target_tokens:
            return content
        
        # æ›´æ™ºèƒ½çš„æˆªæ–­ç­–ç•¥
        # 1. ä¿ç•™å®Œæ•´çš„å¼€å§‹æ ‡è®°
        start_marker_end = content.find("\n\n") + 2
        if start_marker_end < 10:
            start_marker_end = content.find("[START]") + 50  # fallback
        
        # 2. ä¿ç•™å®Œæ•´çš„ç»“æŸæ ‡è®°
        end_marker_start = content.rfind("[END]")
        if end_marker_start == -1:
            end_marker_start = len(content) - 100  # fallback
        
        # 3. è®¡ç®—éœ€è¦ä¿ç•™çš„å†…å®¹é•¿åº¦
        if self.use_tokenizer and self.tokenizer:
            # ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œç²¾ç¡®è°ƒæ•´
            left, right = start_marker_end, end_marker_start
            best_content = content
            
            while left < right:
                mid = (left + right + 1) // 2
                test_content = content[:mid] + content[end_marker_start:]
                test_tokens = self._count_tokens(test_content)
                
                if test_tokens <= target_tokens:
                    best_content = test_content
                    left = mid
                else:
                    right = mid - 1
            
            return best_content
        else:
            # å­—ç¬¦ä¼°ç®—æ–¹å¼ï¼Œä½†æ›´æ™ºèƒ½
            estimated_chars = int(target_tokens * self.chars_per_token)
            
            # è®¡ç®—å†…å®¹éƒ¨åˆ†çš„å¤§è‡´ä½ç½®
            content_start = start_marker_end
            content_end = end_marker_start
            content_length = content_end - content_start
            
            if content_length > 0:
                # æŒ‰æ¯”ä¾‹ç¼©æ”¾å†…å®¹
                scale = min(1.0, estimated_chars / len(content))
                new_content_end = int(content_start + content_length * scale)
                
                # å¯»æ‰¾åˆé€‚çš„æˆªæ–­ç‚¹ï¼ˆå¥å­è¾¹ç•Œï¼‰
                truncated_content = content[content_start:new_content_end]
                
                # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
                sentence_endings = ['. ', '! ', '? ', 'ã€‚\n', 'ï¼\n', 'ï¼Ÿ\n', '\n\n']
                best_pos = 0
                for ending in sentence_endings:
                    pos = truncated_content.rfind(ending)
                    if pos > best_pos:
                        best_pos = pos + len(ending)
                
                if best_pos > 0:
                    final_content = content[:content_start + best_pos] + content[end_marker_start:]
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å¥å­è¾¹ç•Œï¼Œåœ¨å•è¯è¾¹ç•Œæˆªæ–­
                    word_boundary = truncated_content.rfind(' ')
                    if word_boundary > 0:
                        final_content = content[:content_start + word_boundary] + content[end_marker_start:]
                    else:
                        final_content = content[:new_content_end] + content[end_marker_start:]
                
                return final_content
            
            # fallback
            return content[:estimated_chars] + content[end_marker_start:]
    
    def _save_test_content(self, size_name: str, test_content: str, random_numbers: List[int]) -> None:
        """ä¿å­˜æµ‹è¯•å†…å®¹åˆ°logç›®å½•"""
        try:
            # åˆ›å»ºlogç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "log")
            os.makedirs(log_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³å’Œç”¨æˆ·è¾“å…¥çš„å¤§å°ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # å°†å¤šä¸ªéšæœºæ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºæ–‡ä»¶å
            random_str = "_".join(map(str, random_numbers))
            filename = f"context_test_{timestamp}_{size_name}_random_{random_str}.txt"
            filepath = os.path.join(log_dir, filename)
            
            # å†™å…¥æ–‡ä»¶ï¼ˆåªä¿å­˜æµ‹è¯•å†…å®¹ï¼‰
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(test_content)
            
            print(f"   ğŸ“ æµ‹è¯•å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"   âš ï¸ ä¿å­˜æµ‹è¯•å†…å®¹å¤±è´¥: {str(e)}")
    
    def _save_round_content(self, size_name: str, round_num: int, test_content: str, random_numbers: List[int]) -> None:
        """ä¿å­˜æ¯è½®æµ‹è¯•å†…å®¹åˆ°logç›®å½•"""
        try:
            # åˆ›å»ºlogç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "log")
            os.makedirs(log_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ã€ç”¨æˆ·è¾“å…¥çš„å¤§å°å’Œè½®æ•°ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # å°†å¤šä¸ªéšæœºæ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºæ–‡ä»¶å
            random_str = "_".join(map(str, random_numbers))
            filename = f"context_test_{timestamp}_{size_name}_round{round_num + 1}_random_{random_str}.txt"
            filepath = os.path.join(log_dir, filename)
            
            # å†™å…¥æ–‡ä»¶ï¼ˆåªä¿å­˜æµ‹è¯•å†…å®¹ï¼‰
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(test_content)
            
            print(f"      ğŸ“ ç¬¬{round_num + 1}è½®å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            print(f"      âš ï¸ ä¿å­˜ç¬¬{round_num + 1}è½®å†…å®¹å¤±è´¥: {str(e)}")


class APIError(Exception):
    """API ç›¸å…³é”™è¯¯"""
    pass


class NetworkError(Exception):
    """ç½‘ç»œç›¸å…³é”™è¯¯"""
    pass


class ContentGenerationError(Exception):
    """å†…å®¹ç”Ÿæˆé”™è¯¯"""
    pass




def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ä¸Šä¸‹æ–‡çª—å£é•¿åº¦æµ‹è¯•å·¥å…· - æµ‹è¯• LLM æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python context_length_tester.py --key your_api_key_here
  python context_length_tester.py --key your_api_key_here --sizes 32k 64k 128k
  python context_length_tester.py --key your_api_key_here --sizes 256k 512k
  python context_length_tester.py --key your_api_key_here --sizes 64k 128k 192k
  python context_length_tester.py --key your_api_key_here --sizes 90k 150k 300k
  python context_length_tester.py --key your_api_key_here --sizes 50000 100000 150000
  python context_length_tester.py --key your_api_key_here --rounds 3 --timeout 600
  python context_length_tester.py --key your_api_key_here --chat-api
  python context_length_tester.py --key your_api_key_here --output-file results.json
  python context_length_tester.py --key your_api_key_here --max-paragraphs 50000
  python context_length_tester.py --key your_api_key_here --detail
  python context_length_tester.py --key your_api_key_here --query-num 10
  python context_length_tester.py --key your_api_key_here --query-num 10 --detail  # å¯ç”¨è°ƒè¯•ä¿¡æ¯
  python context_length_tester.py --key your_gemini_api_key_here --gemini-api --model gemini-1.5-pro
  python context_length_tester.py --key your_gemini_api_key_here --gemini-api --sizes 32k 64k 128k
        """
    )
    
    # API é…ç½®å‚æ•°
    parser.add_argument(
        "--key", 
        required=True,
        help="API å¯†é’¥ï¼ˆå¿…éœ€ï¼‰"
    )
    parser.add_argument(
        "--url",
        default=DEFAULT_API_URL,
        help="API æ¥å£åœ°å€ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    parser.add_argument(
        "--model",
        default=DEFAULT_MODEL,
        help="ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé»˜è®¤ï¼š%(default)sï¼‰"
    )
    
    # æµ‹è¯•å¤§å°å‚æ•°
    parser.add_argument(
        "--sizes",
        nargs="+",
        help="æµ‹è¯•å¤§å°åˆ—è¡¨ï¼Œæ”¯æŒé¢„è®¾å¤§å°ï¼ˆ1k, 2k, 4k, 8k, 16k, 32k, 64k, 128k, 256k, 512kï¼‰æˆ–è‡ªå®šä¹‰å¤§å°ï¼ˆå¦‚ 90k, 192k, 50000ï¼‰ï¼Œkåç¼€è¡¨ç¤ºä¹˜ä»¥1024ï¼Œæ•°å­—èŒƒå›´1-65536"
    )
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument(
        "--rounds",
        type=int,
        default=DEFAULT_TEST_ROUNDS,
        help="æ¯ä¸ªå¤§å°æµ‹è¯•è½®æ•°ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=DEFAULT_TIMEOUT,
        help="å•è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--chars-per-token",
        type=float,
        default=None,
        help="æ¯ä¸ª token çš„å­—ç¬¦æ•°ï¼ˆé»˜è®¤ï¼šè‹±æ–‡4.0ï¼Œä¸­æ–‡2.0ï¼‰"
    )
    parser.add_argument(
        "--chat-api",
        action="store_true",
        help="ä½¿ç”¨ Chat API æ¥å£ï¼ˆé»˜è®¤ï¼šä½¿ç”¨ Anthropic æ¥å£ï¼‰"
    )
    parser.add_argument(
        "--gemini-api",
        action="store_true",
        help="ä½¿ç”¨ Gemini API æ¥å£ï¼ˆé»˜è®¤ï¼šä½¿ç”¨ Anthropic æ¥å£ï¼‰"
    )
    parser.add_argument(
        "--output-file",
        type=str,
        help="å¯¼å‡ºç»“æœåˆ° JSON æ–‡ä»¶"
    )
    parser.add_argument(
        "--max-paragraphs",
        type=int,
        default=DEFAULT_MAX_PARAGRAPHS,
        help="æœ€å¤§æ®µè½æ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    parser.add_argument(
        "--use-english",
        action="store_true",
        help="ä½¿ç”¨è‹±æ–‡ç”Ÿæˆpromptå†…å®¹ï¼ˆé»˜è®¤ä½¿ç”¨ä¸­æ–‡ï¼‰"
    )
    parser.add_argument(
        "--disable-thinking",
        action="store_true",
        help="ç¦ç”¨ GLM æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ï¼ˆé»˜è®¤ä¼šè‡ªåŠ¨ä¸º GLM æ¨¡å‹ç¦ç”¨ï¼‰"
    )
    parser.add_argument(
        "--detail",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†çš„å“åº”å†…å®¹ï¼ˆé»˜è®¤ä¸æ˜¾ç¤ºï¼‰"
    )
    parser.add_argument(
        "--query-num",
        type=int,
        default=1,
        help="æ’å…¥çš„éšæœºæ•°æ•°é‡ï¼ˆé»˜è®¤ï¼š%(default)dï¼‰"
    )
    
    return parser.parse_args()


def parse_custom_size(custom_size_str: str) -> int:
    """è§£æè‡ªå®šä¹‰å¤§å°è¡¨è¾¾å¼ï¼Œæ”¯æŒæ•°å­—+kçš„å½¢å¼
    
    Args:
        custom_size_str: è‡ªå®šä¹‰å¤§å°å­—ç¬¦ä¸²ï¼ˆå¦‚ "192k", "90k", "50000"ï¼‰
    
    Returns:
        è§£æåçš„tokenæ•°é‡
    
    Raises:
        ValueError: å½“æ ¼å¼æ— æ•ˆæˆ–æ•°å­—è¶…å‡ºèŒƒå›´æ—¶
    """
    import re
    
    # åŒ¹é…æ•°å­— + å¯é€‰çš„ k åç¼€
    pattern = r'^(\d+)(k?)$'
    match = re.match(pattern, custom_size_str.lower())
    
    if not match:
        raise ValueError(f"æ— æ•ˆçš„è‡ªå®šä¹‰å¤§å°æ ¼å¼: {custom_size_str}ã€‚åº”ä¸ºæ•°å­—æˆ–æ•°å­—+kï¼ˆå¦‚ 90kã€192kã€50000ï¼‰")
    
    number = int(match.group(1))
    suffix = match.group(2)
    
    # éªŒè¯æ•°å­—èŒƒå›´
    if number < 1 or number > 65536:
        raise ValueError(f"æ•°å­—å¿…é¡»åœ¨1-65536èŒƒå›´å†…ï¼Œå½“å‰å€¼: {number}")
    
    # æ ¹æ®åç¼€è®¡ç®—æœ€ç»ˆå€¼
    if suffix == 'k':
        return number * 1024  # ä½¿ç”¨1024è€Œä¸æ˜¯1000ï¼Œä¸PRESET_SIZESä¿æŒä¸€è‡´
    else:
        return number


def main():
    """ä¸»å‡½æ•° - è¿è¡Œä¸Šä¸‹æ–‡é•¿åº¦æµ‹è¯•"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    api_url = args.url
    
    tester = ContextLengthTester(
        api_url=api_url,
        api_key=args.key,
        model=args.model,
        test_sizes=args.sizes,
        test_rounds=args.rounds,
        timeout=args.timeout,
        chars_per_token=args.chars_per_token,
        use_chat_api=args.chat_api,
        use_gemini_api=args.gemini_api,
        output_file=args.output_file,
        max_paragraphs=args.max_paragraphs,
        use_english=args.use_english,
        disable_thinking=args.disable_thinking,
        show_detail=args.detail,
        query_num=args.query_num
    )
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_test()
    
    return results


if __name__ == "__main__":
    main()
